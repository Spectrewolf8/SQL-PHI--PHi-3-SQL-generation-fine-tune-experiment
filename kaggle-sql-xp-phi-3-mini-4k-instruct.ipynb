{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install flash_attn==2.5.8\n!pip install torch==2.3.1\n!pip install accelerate==0.31.0\n!pip install transformers==4.41.2\n!pip install datasets\n!pip install transformers\n!pip install trl\n!pip install peft \n!pip install auto-gptq \n!pip install optimum\n!pip install xformers\n!pip install huggingface_hub\n!pip install git+https://github.com/microsoft/LoRA\n    \n#bits and bytes with cuda\n!pip install bitsandbytes-cuda110 bitsandbytes","metadata":{"_uuid":"4d55c2d0-72b2-4fc6-b00d-a2a2fce43fcd","_cell_guid":"b8a7dcd1-aa44-44d7-aebe-d76376affb56","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Important: You must restart the kernel at this point after installing the packages!!\n`This notebook is written and targetted towards kaggle`\n\n---","metadata":{"_uuid":"a933ad58-25a7-4b8f-90ae-7f6aedca5231","_cell_guid":"56b40174-8d43-432e-a6e8-1b45c4f41aa6","trusted":true}},{"cell_type":"markdown","source":"# Preparing datasets, loading model and tokenizer, Training model \n### Model used: microsoft/Phi-3-mini-4k-instruct","metadata":{"_uuid":"5e4648e6-add2-46bb-bfae-08d491ae2488","_cell_guid":"d1f04aad-873d-4936-aad5-f8f01b4bd74b","trusted":true}},{"cell_type":"code","source":"#load tokens\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\n\n#logging into Hugging Face\n!huggingface-cli login --token $hf_token","metadata":{"_uuid":"58d11453-8740-483c-b03c-fa0f16655c6a","_cell_guid":"d3f931aa-165a-424f-96e1-0d78ee0aba12","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# impoting classes\nfrom random import randrange\n\nimport torch\nfrom datasets import load_dataset\n\nfrom peft import LoraConfig, prepare_model_for_kbit_training, PeftModel\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    set_seed,\n    pipeline\n)\nfrom trl import SFTTrainer","metadata":{"_uuid":"9a275092-7607-4cf1-99f2-65e59bfb087a","_cell_guid":"127a0214-0fab-4775-b206-4bccf2acc09a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preparing datasets\n\n# DATASET_NAME is a string that specifies the name of the dataset to be used for fine-tuning.\nDATASET_NAME = synthetic_text_to_sql_dataset_name = \"gretelai/synthetic_text_to_sql\"\n\n# Load the dataset specified by DATASET_NAME using the load_dataset function.\ndataset = load_dataset(DATASET_NAME)\n\ndataset\n\n# Extract relevant fields\n\n# old\n# def extract_fields_synthetic(example):\n#     return {\n#         \"question\": example[\"sql_prompt\"],\n#         \"context\": example[\"sql_context\"],\n#         \"sql\": example[\"sql\"]\n#     }\n\n# new\ndef extract_fields_synthetic(example):\n    return {\n        \"instruction\": example[\"sql_prompt\"],\n        \"input\": example[\"sql_context\"],\n        \"output\": example[\"sql\"]\n    }\nsynthetic_extracted_dataset = dataset.map(extract_fields_synthetic, remove_columns=dataset['train'].column_names)","metadata":{"_uuid":"e81cad84-660b-403c-be37-ef7bb674d9cf","_cell_guid":"d8cce8bc-958b-43a8-9243-ce7384004270","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split and shuffle datasets\n\nimport random \n\nsynthetic_extracted_train_dataset = synthetic_extracted_dataset[\"train\"]\nsynthetic_extracted_test_dataset = synthetic_extracted_dataset[\"test\"]\n\n# Shuffle the dataset\nsynthetic_extracted_dataset = synthetic_extracted_dataset.shuffle(seed=random.randint(10,99))\nsynthetic_extracted_dataset = synthetic_extracted_dataset.shuffle(seed=random.randint(10,99))\n\nprint(synthetic_extracted_train_dataset)\nprint(synthetic_extracted_test_dataset)","metadata":{"_uuid":"0ed8c885-9d54-41d4-810d-c3f715bedfa1","_cell_guid":"35a44a41-4d7a-4be6-bf11-94017f1db749","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'torch.cuda.is_bf16_supported()' is a function that checks if BF16 is supported on the current GPU. BF16 is a data type that uses 16 bits, like float16, but allocates more bits to the exponent, which can result in higher precision.\n# 'attn_implementation' is a variable that will hold the type of attention implementation to be used.\n\nif torch.cuda.is_bf16_supported():\n  compute_dtype = torch.bfloat16\nelse:\n  compute_dtype = torch.float16\n\nattn_implementation = 'eager'\nprint(attn_implementation)\nprint(compute_dtype)","metadata":{"_uuid":"c6651fc7-3838-4b07-a706-c7583feba83d","_cell_guid":"2bcda16f-49eb-44b7-975f-ae6251984f34","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MODEL_ID is a string that specifies the identifier of the pre-trained model that will be fine-tuned. \nMODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n\n# NEW_MODEL_NAME is a string that specifies the name of the new model after fine-tuning.\nNEW_MODEL_NAME = \"sql-xp-phi-3-mini-4k\"","metadata":{"_uuid":"47b3096d-3b57-4f8d-a2b6-006710509190","_cell_guid":"684cb7a2-c992-4503-b1e6-d229bc954436","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load tokenizr to prepare dataset\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\ntokenizer.padding_side = 'right' # to prevent warnings","metadata":{"_uuid":"82fbc585-0a8c-4658-b83a-a35afa6a2aa5","_cell_guid":"9ecd61b8-5a9a-4953-b9d5-0bf007d4afd3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define methods for creating and formatting messages/prompts for datasets\n# The prompt will contain our instructions, and the context will include the SQL context, such as a table creation SQL command\n\ndef create_message_column(row):\n    \"\"\"\n    Create a message column for a dataset row.\n    Args: row (dict): A dictionary containing 'instruction', 'input', and 'output' keys.\n    Returns: dict: A dictionary with the key 'message' containing a list of messages for user and assistant.\n    \"\"\"\n    message = []\n\n    # Define the user message with prompt and context\n    user = {\"content\": f\"\\n #prompt: {row['instruction']}\\n #context: {row['input']}\", \"role\": \"user\"}\n    message.append(user)\n\n    # Define the assistant's response\n    assistant = {\"content\": f\"{row['output']}\",\"role\": \"assistant\"}\n    message.append(assistant)\n\n    # Return the constructed message\n    return {\"message\": message}\n\ndef format_dataset_with_chat_template(row):\n    \"\"\"\n    Format a dataset row using the chat template for tokenization.\n    Args: row (dict): A dictionary containing the 'message' key.\n    Returns:dict: A dictionary with the key 'text' containing the formatted text.\n    \"\"\"\n    # Apply the chat template to the message and return the formatted text\n    return {\"text\": tokenizer.apply_chat_template(row[\"message\"], add_generation_prompt=False, tokenize=False)}","metadata":{"_uuid":"17449f9f-15df-4b2f-b553-5332f9a69205","_cell_guid":"5384c2e8-d721-472a-9448-caafa88b3b53","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply create_message_column function\nsynthetic_extracted_train_dataset = synthetic_extracted_train_dataset.map(create_message_column)\nsynthetic_extracted_test_dataset = synthetic_extracted_test_dataset.map(create_message_column)\n\n# Format dataset using \nsynthetic_extracted_train_dataset = synthetic_extracted_train_dataset.map(format_dataset_with_chat_template)\nsynthetic_extracted_test_dataset = synthetic_extracted_test_dataset.map(format_dataset_with_chat_template)\n\n# Output the results to verify\nprint(synthetic_extracted_train_dataset)\nprint(synthetic_extracted_test_dataset)","metadata":{"_uuid":"e2463809-8765-4020-b149-9ffcb454b4e1","_cell_guid":"d7abff41-7819-47e7-bad8-f2b445911840","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#select subsets of datasets\n# 75:25 dataset ratio\n\nsynthetic_extracted_train_dataset = synthetic_extracted_train_dataset.select(range(10000))\nsynthetic_extracted_test_dataset = synthetic_extracted_test_dataset.select(range(3300))","metadata":{"_uuid":"6b097804-6ff8-4e33-b1b4-5b24b5662931","_cell_guid":"e7cd3b76-4f1d-435d-bf30-877dc00d8e3c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'hf_model_repo' is the identifier for the Hugging Face repository where you want to save the fine-tuned model.\nhf_model_repo=\"spectrewolf8/\"+NEW_MODEL_NAME\n\n# Load Model on GPU \n# 'device_map' is set to {\"\": 0}, which means that the entire model will be loaded on GPU 0.\ndevice_map = {\"\": 0}\n\n# Bits and Bytes configuration for the model\n\n# 'load_in_4bit' is a boolean that control if 4bit quantization should be loaded. In this case, it is set to True\n# 'bnb_4bit_compute_dtype' is the data type that should be used for computations with the 4-bit base model. In this case, it is set to 'bfloat16'.\n# 'bnb_4bit_quant_type' is the type of quantization that should be used for the 4-bit base model. In this case, it is set to 'nf4'.\n# 'bnb_4bit_use_double_quant' is a boolean that controls whether nested quantization should be used for the 4-bit base model.\n\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=True,\n)\n\n# LoRA configuration for the model\n\n# 'lora_r' or 'r' is the dimension of the LoRA attention.\n# 'lora_alpha' is the alpha parameter for LoRA scaling.\n# 'lora_dropout' is the dropout probability for LoRA layers.\n# 'target_modules' is a list of the modules that should be targeted by LoRA.\n# peft configuration for the model\n\nlora_r = 16 #16 default\npeft_config = LoraConfig(\n    lora_alpha = 16, #16 default\n    lora_dropout = 0.05, #0.05 default\n    r = lora_r,\n    target_modules=['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"]\n)","metadata":{"_uuid":"0a30532f-2411-4932-8a18-dcf3e689373b","_cell_guid":"39788e3b-b879-4168-befc-cce20b0c50ed","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # 'set_seed(1234)' sets the random seed for reproducibility.\n# set_seed(1234)\n\n# # username is a string that specifies the GitHub username of the person who is fine-tuning the model.\n# # license is a string that specifies the license under which the model is distributed. In this case, it's Apache License 2.0.\n\n# username = \"spectrewolf8\"\n# license = \"apache-2.0\"","metadata":{"_uuid":"dcc3e338-34eb-4506-964a-5ced1907eb89","_cell_guid":"e51bab17-7c7d-4787-b87f-af18f60c0008","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'AutoTokenizer' is a class from the Hugging Face Transformers library that provides a tokenizer for a given pre-trained model.\n# 'from_pretrained' is a method of the 'AutoTokenizer' class that loads a tokenizer from a pre-trained model.\n# 'trust_remote_code=True' is a parameter that allows the execution of remote code when loading the tokenizer.\n# 'add_eos_token=True' is a parameter that adds an end-of-sentence token to the tokenizer.\n# 'use_fast=True' is a parameter that uses the fast version of the tokenizer, if available.\n# 'tokenizer.pad_token = tokenizer.unk_token' sets the padding token of the tokenizer to be the same as the unknown token.\n# 'tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)' sets the ID of the padding token to be the same as the ID of the padding token.\n# 'tokenizer.padding_side = 'left'' sets the side where padding will be added to be the left side.\n# 'BitsAndBytesConfig' is a class that provides a configuration for quantization.\n# 'bnb_config' is a variable that holds the configuration for quantization.\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True, add_eos_token=True, use_fast=True)\ntokenizer.pad_token = tokenizer.unk_token\ntokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\ntokenizer.padding_side = 'left'\n\n# 'AutoModelForCausalLM' is a class from the Hugging Face Transformers library that provides a model for causal language modeling.\n# 'from_pretrained' is a method of the 'AutoModelForCausalLM' class that loads a model from a pre-trained model.\n# 'torch_dtype=compute_dtype' is a parameter that sets the data type of the model to be the same as 'compute_dtype'.\n# 'quantization_config=bnb_config' is a parameter that sets the configuration for quantization to be 'bnb_config'.\n# 'device_map=device_map' is a parameter that sets the device map of the model to be 'device_map'.\n# 'attn_implementation=attn_implementation' is a parameter that sets the type of attention implementation to be 'attn_implementation'.\n# 'model = prepare_model_for_kbit_training(model)' prepares 'model' for k-bit training and assigns the result back to 'model'.\n\nmodel = AutoModelForCausalLM.from_pretrained(\n          MODEL_ID, torch_dtype=compute_dtype, trust_remote_code=True, quantization_config=bnb_config, device_map=device_map,\n          attn_implementation=attn_implementation\n)\nmodel = prepare_model_for_kbit_training(model)\nmodel.gradient_checkpointing_enable()","metadata":{"_uuid":"87706bdb-a705-44f5-bd9e-8a6446959962","_cell_guid":"d54bec1d-283c-4244-9c9d-f1fa722cbb41","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This block of code is used to initialize Weights & Biases (wandb) for experiment tracking.\n\n# Retrieve the Weights & Biases API token from user secrets\nwandb_token = user_secrets.get_secret(\"WANDB_TOKEN\")\n\n# Import the wandb library for experiment tracking\nimport wandb\n\n# Log in to Weights & Biases using the retrieved API token\nwandb.login(key=wandb_token)\n\n# Initialize a new Weights & Biases run for tracking the experiment\nrun = wandb.init(\n    project='Training and tuning Phi-3-mini-4k-instruct for SQL | kaggle-sql-xp-phi-3-mini-4k-instruct.ipynb', \n    job_type=\"training\",  # Specify the type of job as training\n    anonymous=\"allow\"     # Allow anonymous logging if no user is logged in\n)","metadata":{"_uuid":"fc2f6ab2-9ace-4520-9f86-14dc166f3f48","_cell_guid":"e0da8460-21d8-4876-9578-55674bc69069","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training model","metadata":{"_uuid":"1777d983-443e-42b4-81a3-5e72b1be975b","_cell_guid":"2f2d972c-a59b-41fe-aa6a-e191e4869dc9","trusted":true}},{"cell_type":"code","source":"# 'TrainingArguments' is a class from the Hugging Face Transformers library that provides hyperparameters for training.\n# 'output_dir=\"./results\"' sets the directory where the training results (like checkpoints and logs) will be saved.\n# 'num_train_epochs=1' sets the number of times the entire training dataset will be passed through the model.\n# 'per_device_train_batch_size=4' sets the batch size for training on each device (e.g., GPU).\n# 'gradient_accumulation_steps=1' sets the number of steps to accumulate gradients before performing a backward/update pass.\n# 'optim=\"paged_adamw_32bit\"' specifies the optimizer to use; in this case, \"paged_adamw_32bit\" is used.\n# 'save_steps=25' specifies the number of steps before saving a checkpoint.\n# 'logging_steps=10' specifies the number of steps before logging training metrics.\n# 'learning_rate=2e-4' sets the learning rate for the optimizer.\n# 'weight_decay=0.001' applies weight decay (L2 regularization) to prevent overfitting.\n# 'fp16=False' specifies whether to use 16-bit (half-precision) floating point.\n# 'bf16=False' specifies whether to use bfloat16 precision (an alternative to fp16).\n# 'max_grad_norm=0.3' clips the gradient norm to prevent the exploding gradient problem.\n# 'max_steps=-1' specifies the total number of training steps; -1 means no limit.\n# 'warmup_ratio=0.03' sets the proportion of training steps to perform learning rate warmup.\n# 'group_by_length=True' groups sequences of similar lengths together for efficient training.\n# 'lr_scheduler_type=\"constant\"' specifies the type of learning rate scheduler; in this case, it uses a constant learning rate.\n# 'report_to=\"wandb\"' specifies the reporting tool to use for logging; in this case, Weights and Biases (wandb) is used.\n\ntraining_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=2,#1 default\n    per_device_train_batch_size=4,#4 default\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    save_steps=25,\n    logging_steps=5,#10 default but that's just for logging\n    learning_rate=2e-4,#1.41e-5 default\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,#False default\n    max_grad_norm=0.3,#0.3 default\n    max_steps=-1,\n    warmup_ratio=0.03,#0.03 default\n    group_by_length=True,\n    lr_scheduler_type=\"linear\",\n    report_to=\"wandb\"\n)","metadata":{"_uuid":"87479761-4a28-4a97-a683-e392dda7a580","_cell_guid":"1ab63e32-bf84-4aae-9f4a-88a54d842501","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'SFTTrainer' is a class that provides a trainer for fine-tuning a model.\n# 'trainer' is a variable that holds the trainer.\n# 'model=model' is a parameter that sets the model to be trained to be 'model'.\n# 'train_dataset=synthetic_extracted_train_dataset' is a parameter that sets the training dataset to be 'synthetic_extracted_train_dataset'.\n# 'eval_dataset=synthetic_extracted_test_dataset' is a parameter that sets the evaluation dataset to be 'synthetic_extracted_test_dataset'.\n# 'peft_config=peft_config' is a parameter that sets the configuration for the Lora layer to be 'peft_config'.\n# 'dataset_text_field=\"text\"' is a parameter that sets the field in the dataset that contains the text to be 'text'.\n# 'max_seq_length=512' is a parameter that sets the maximum sequence length for the model to be 512.\n# 'tokenizer=tokenizer' is a parameter that sets the tokenizer to be 'tokenizer'.\n# 'args=args' is a parameter that sets the training arguments to be 'args'.\n# This line of code is used to create a trainer for fine-tuning the model with the specified parameters.\n\ntrainer = SFTTrainer(\n        model=model,\n        train_dataset=synthetic_extracted_train_dataset,\n        eval_dataset=synthetic_extracted_test_dataset,\n        peft_config=peft_config,\n        dataset_text_field=\"text\",\n        max_seq_length=512, #512 default\n        tokenizer=tokenizer,\n        args=training_arguments,\n)","metadata":{"_uuid":"31c3e042-bd30-4270-9a8f-7736a3cc43da","_cell_guid":"22a2ccf6-c9ca-4dd8-bf81-474daadccf12","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'trainer.train()' is a method that starts the training of the model. It uses the training dataset, model, and training arguments that were specified when the trainer was created.\n\n# train the model\ntrainer.train()","metadata":{"_uuid":"ab70d953-c592-4c4f-b0cd-b19b96402bdb","_cell_guid":"c6bc6026-4ca3-4823-bac4-4846fe336733","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'trainer.save_model()' is a method that saves the trained model to the local file system. The model will be saved in the output directory that was specified in the training arguments.\n# This block of code is used to train the model and then save the trained model to the local file system.\n\n# save model locally\ntrainer.save_model()\ntokenizer.save_pretrained(\"./results\")","metadata":{"_uuid":"3ed6f9c4-aed8-4fa5-bc92-25116218c159","_cell_guid":"51dcb105-cebd-485a-be28-8a9f161f1fcd","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainer.save_model(\"./path_to_save_model\")  # Save the model locally to specified directory\n# tokenizer.save_pretrained(\"./path_to_save_model\")  # Save the tokenizer to specified directory","metadata":{"_uuid":"bd666fd2-3d57-4e67-a085-4c6e39068359","_cell_guid":"e17a7304-5833-44c4-9f48-d0d6bc851984","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the repository name on the Hugging Face Hub where the model, trainer, and tokenizer will be pushed.\nhf_model_repo = \"spectrewolf8/sql-xp-phi-3-mini-4k\"\n\n# Push the trainer to the Hugging Face Hub.\n# This includes training arguments, optimizer states, and other relevant information.\ntrainer.push_to_hub(hf_model_repo)\n\n# Push the model to the Hugging Face Hub.\n# This saves the model weights and configuration to the specified repository.\ntrainer.model.push_to_hub(hf_model_repo)\n\n# Push the tokenizer to the Hugging Face Hub.\n# This saves the tokenizer configuration and vocab files to the specified repository.\ntokenizer.push_to_hub(hf_model_repo)","metadata":{"_uuid":"71364503-7745-423c-9d82-4998683fbe40","_cell_guid":"c02c9731-2790-49af-80a9-879c637cf42b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model stats","metadata":{"_uuid":"7ae767f7-6b10-445c-9667-8aaea5ac2a00","_cell_guid":"54aebe87-4764-4873-a420-35f60d82e1b1","trusted":true}},{"cell_type":"code","source":"# Finish the Weights & Biases (wandb) run.\n# This finalizes the current experiment run, ensuring all data is uploaded and the run is properly closed.\nwandb.finish()\n\n# Set the 'use_cache' configuration option of the model to True.\n# This enables caching of the computation results during inference, which can speed up the model's performance.\nmodel.config.use_cache = True\n\n# Set the model to evaluation mode.\n# This changes the model's behavior to inference mode, disabling features like dropout that are only used during training.\nmodel.eval()","metadata":{"_uuid":"032a4cbd-7eba-47d3-8bf1-d2265294d421","_cell_guid":"941f8584-d155-409b-ac04-261cb987867c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing model","metadata":{"_uuid":"e9711239-bc7b-44d1-a494-c1fd2493ec97","_cell_guid":"edc9daff-f2bc-4772-9d65-a89a27fd4f6c","trusted":true}},{"cell_type":"code","source":"# Create a text generation pipeline using the specified model and tokenizer.\n# The 'pipeline' function sets up a ready-to-use text generation pipeline, combining the model and tokenizer.\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)","metadata":{"_uuid":"5647df58-3917-4969-b987-e14e5c707f6d","_cell_guid":"ac22cd01-0504-4192-93ed-9b79037c4f05","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## example-1","metadata":{"_uuid":"721f18d3-a898-476d-9220-4ac5aa28932c","_cell_guid":"9ea5f6e3-8681-4a6b-8149-21dbbf679b59","trusted":true}},{"cell_type":"code","source":"# Define the input phrase which represents the user's request or query.\ninput_phrase = \"\"\"\ninsert 5 values\n\"\"\"\n\n# Define the context phrase which provides the SQL table schema relevant to the input phrase.\ncontext_phrase = \"\"\"\nCREATE TABLE tasks (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    task_name VARCHAR(100) NOT NULL,\n    userid INT NOT NULL,\n    date DATE NOT NULL,\n    FOREIGN KEY (userid) REFERENCES users(id)\n);\n\"\"\"\n\n# Create a prompt by applying a chat template to the input and context phrases using the tokenizer.\n# The 'apply_chat_template' method formats the input as a chat message, making it suitable for text generation.\n# 'tokenize=False' indicates that the input should not be tokenized yet.\n# 'add_generation_prompt=True' adds a prompt for text generation.\nprompt = pipe.tokenizer.apply_chat_template(\n    [{\"role\": \"user\", \"content\": f\"\\n #prompt: {input_phrase}\\n #context: {context_phrase}\"}],\n    tokenize=False,\n    add_generation_prompt=True\n)\n\n# Generate text using the pipeline with the specified parameters.\n# 'max_new_tokens=256' sets the maximum number of new tokens to generate.\n# 'do_sample=True' enables sampling for text generation.\n# 'num_beams=1' specifies the number of beams for beam search (1 means no beam search).\n# 'temperature=0.3' controls the randomness of predictions by scaling the logits before applying softmax.\n# 'top_k=50' considers only the top 50 token predictions for sampling.\n# 'top_p=0.95' enables nucleus sampling, considering tokens that have a cumulative probability of 0.95.\n# 'max_time=180' sets the maximum generation time to 180 seconds.\noutputs = pipe(\n    prompt,\n    max_new_tokens=256,\n    do_sample=True,\n    num_beams=1,\n    temperature=0.3,\n    top_k=50,\n    top_p=0.95,\n    max_time=180\n)\n\n# Print the generated text by stripping out the prompt portion and displaying only the new generated content.\nprint(outputs[0]['generated_text'][len(prompt):].strip())","metadata":{"_uuid":"d1d19218-0f5a-4a49-a702-6b13ff4c5292","_cell_guid":"3d1202a6-4e9d-459e-aa1a-dd5cb2e623ea","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## example-2","metadata":{"_uuid":"fe67186a-d567-4863-b3de-5f45b9fd7e78","_cell_guid":"01f13aab-731e-4992-8851-10b9f3c6582b","trusted":true}},{"cell_type":"code","source":"# Define a list of input phrases representing various SQL operations.\ninput_phrases = [\n    \"insert 5 values\",\n    \"select all records\",\n    \"update record with id 3\",\n    \"delete all records where task_name is 'coding'\",\n    \"add a new column 'status' to the table\",\n    \"find all tasks with userid 2\",\n    \"count the number of tasks per user\",\n    \"list all tasks sorted by date\",\n    \"join tasks with users\",\n    \"find the average number of tasks per user\"\n]\n\n# Define a list of context phrases which provide the SQL table schema.\n# The same context is used for all input phrases in this example.\ncontext_phrases = [\n    \"\"\"\n    CREATE TABLE tasks (\n        id INT AUTO_INCREMENT PRIMARY KEY,\n        name VARCHAR(100) NOT NULL,\n        task_name VARCHAR(100) NOT NULL,\n        userid INT NOT NULL,\n        date DATE NOT NULL,\n        FOREIGN KEY (userid) REFERENCES users(id)\n    );\n    \"\"\"\n] * len(input_phrases)  # Repeat the same context for all input phrases.\n\n# Apply the chat template to create prompts by combining input and context phrases.\n# The 'apply_chat_template' method formats each input phrase with its corresponding context phrase.\n# f\"\\n #prompt: {input_phrase}\\n #context: {context_phrase}\"\n\nprompts = [pipe.tokenizer.apply_chat_template(\n    [{\"role\": \"user\", \"content\": f\"\\n #prompt: {input_phrase}\\n #context: {context_phrase}\"}], \n    tokenize=False, \n    add_generation_prompt=True\n) for input_phrase, context_phrase in zip(input_phrases, context_phrases)]\n\n# Generate SQL queries using the text generation pipeline with specified parameters.\n# Each prompt is passed through the pipeline to generate the corresponding SQL query.\noutputs = [pipe(\n    prompt, \n    max_new_tokens=256, \n    do_sample=True, \n    num_beams=1, \n    temperature=0.3, \n    top_k=50, \n    top_p=0.95, \n    max_time=180\n) for prompt in prompts]\n\n# Print the results of the generated SQL queries.\n# For each generated output, strip out the prompt portion and display only the new generated content.\nfor i, output in enumerate(outputs):\n    generated_text = output[0]['generated_text'][len(prompts[i]):].strip()\n    print(f\"Prompt {i+1}:\")\n    print(generated_text)\n    print(\"\\n\")","metadata":{"_uuid":"d903567e-38ac-462f-8b3a-272fb67dc034","_cell_guid":"571a74b6-309b-4bb8-9dd7-8cba97ec73c6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading model from hugging face","metadata":{"_uuid":"67e85d38-4967-46bd-a496-90a917656249","_cell_guid":"84c83b01-4139-4968-b91a-a47a319a2c47","trusted":true}},{"cell_type":"code","source":"# Import necessary libraries\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n\n# Set the seed for the random number generator to ensure reproducibility\nset_seed(1234)\n\n# Define the repository name for the Hugging Face model\n# 'hf_model_repo' is a variable that holds the repository name for the Hugging Face model\n# 'username/modelname' is the repository name, where 'username' is the username of the repository owner\n# and 'modelname' is the name of the model\nhf_model_repo = \"spectrewolf8/sql-xp-phi-3-mini-4k\"\n\n# Retrieve the device mapping and computation data type\n# 'device_map' is a variable that holds the mapping of the devices that are used for computation\n# 'compute_dtype' is a variable that holds the data type that is used for computation\n\n# device_map = {\"\": 0}\n# compute_dtype = torch.bfloat16 or torch.float16\ndevice_map, compute_dtype\n\n# Load a pre-trained tokenizer from the Hugging Face Model Hub\n# 'tokenizer' is the variable that holds the tokenizer\n# 'trust_remote_code=True' allows the execution of code from the model file\ntokenizer = AutoTokenizer.from_pretrained(hf_model_repo, trust_remote_code=True)\n\n# Load a pre-trained model for causal language modeling from the Hugging Face Model Hub\n# 'model' is the variable that holds the model\n# 'trust_remote_code=True' allows the execution of code from the model file\n# 'torch_dtype=compute_dtype' sets the data type for the PyTorch tensors\n# 'device_map=device_map' sets the device mapping\nmodel = AutoModelForCausalLM.from_pretrained(hf_model_repo, trust_remote_code=True, torch_dtype=compute_dtype, device_map=device_map)","metadata":{"_uuid":"2cf9c76a-1225-4514-a8da-4397500bf364","_cell_guid":"0f3e55fa-cbd9-4e7b-a11b-399ee368c8e5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)","metadata":{"_uuid":"08031757-fe3d-4ddd-9aaa-4b731ff3dc88","_cell_guid":"e14a8cfd-387a-411c-9a8f-45a89bf631f2","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Rest of the steps from here are the same as example-1 and example-2","metadata":{"_uuid":"c9234054-fe16-45ed-803a-58301c68bfac","_cell_guid":"155c04bd-9753-4574-b283-5b209ee972ca","trusted":true}},{"cell_type":"code","source":"# Define the context and input phrase\ncontext_phrase = \"\"\"\nCREATE TABLE users (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    username VARCHAR(50) NOT NULL,\n    email VARCHAR(100) NOT NULL,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE TABLE projects (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    project_name VARCHAR(100) NOT NULL,\n    description TEXT,\n    start_date DATE NOT NULL,\n    end_date DATE,\n    user_id INT,\n    FOREIGN KEY (user_id) REFERENCES users(id)\n);\n\nCREATE TABLE tasks (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    task_name VARCHAR(100) NOT NULL,\n    status VARCHAR(20) CHECK (status IN ('pending', 'in_progress', 'completed')),\n    priority INT CHECK (priority BETWEEN 1 AND 5),\n    project_id INT,\n    assigned_to INT,\n    due_date DATE,\n    FOREIGN KEY (project_id) REFERENCES projects(id),\n    FOREIGN KEY (assigned_to) REFERENCES users(id)\n);\n\nCREATE TABLE comments (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    task_id INT,\n    user_id INT,\n    comment_text TEXT NOT NULL,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (task_id) REFERENCES tasks(id),\n    FOREIGN KEY (user_id) REFERENCES users(id)\n);\n\"\"\"\n\ninput_phrase = \"\"\"\nUpdate the status of tasks to 'completed' for all tasks that have passed their due date. Also, update the end date of the corresponding projects to the current date if all tasks in the project are completed.\n\"\"\"\n\n# Apply the chat template to create the prompt\nprompt = pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": f\"\\n #prompt: {input_phrase}\\n #context: {context_phrase}\"}], tokenize=False, add_generation_prompt=True)\n\n# Generate SQL query\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, num_beams=1, temperature=0.3, top_k=50, top_p=0.95, max_time=180)\n\n# Print the result\ngenerated_text = outputs[0]['generated_text'][len(prompt):].strip()\nprint(f\"Generated SQL Query:\\n{generated_text}\")","metadata":{"_uuid":"9fab445a-afb9-416e-b6c5-518d9ca00d21","_cell_guid":"292d94f0-0dc6-40e7-a332-8525518d32e4","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(prompt)","metadata":{"_uuid":"20750dd7-73e7-41dd-a6c3-3d2b4c15f7ff","_cell_guid":"0a581f15-ce93-4dcb-8ca3-e07aed60580d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}