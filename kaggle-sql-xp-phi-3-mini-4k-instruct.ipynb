{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install flash_attn==2.5.8\n!pip install torch==2.3.1\n!pip install accelerate==0.31.0\n!pip install transformers==4.41.2\n!pip install datasets\n!pip install transformers\n!pip install trl\n!pip install peft \n!pip install auto-gptq \n!pip install optimum\n!pip install xformers\n!pip install huggingface_hub\n!pip install git+https://github.com/microsoft/LoRA","metadata":{"_uuid":"a2d66a96-2bc5-44f5-a120-4ccfd2a2c5a0","_cell_guid":"4ad0de5d-83a6-46d6-82a1-55410bfcf5d5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-07-18T09:04:27.297079Z","iopub.execute_input":"2024-07-18T09:04:27.297389Z","iopub.status.idle":"2024-07-18T09:07:25.407913Z","shell.execute_reply.started":"2024-07-18T09:04:27.297358Z","shell.execute_reply":"2024-07-18T09:07:25.406674Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting flash_attn==2.5.8\n  Using cached flash_attn-2.5.8-cp310-cp310-linux_x86_64.whl\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash_attn==2.5.8) (2.3.1)\nRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from flash_attn==2.5.8) (0.8.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from flash_attn==2.5.8) (21.3)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from flash_attn==2.5.8) (1.11.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->flash_attn==2.5.8) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn==2.5.8) (3.13.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn==2.5.8) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn==2.5.8) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn==2.5.8) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn==2.5.8) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn==2.5.8) (2024.5.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn==2.5.8) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn==2.5.8) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn==2.5.8) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn==2.5.8) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn==2.5.8) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn==2.5.8) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn==2.5.8) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn==2.5.8) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn==2.5.8) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn==2.5.8) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn==2.5.8) (12.1.105)\nRequirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn==2.5.8) (2.3.1)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash_attn==2.5.8) (12.5.82)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->flash_attn==2.5.8) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash_attn==2.5.8) (1.3.0)\nInstalling collected packages: flash_attn\n  Attempting uninstall: flash_attn\n    Found existing installation: flash-attn 2.6.1\n    Uninstalling flash-attn-2.6.1:\n      Successfully uninstalled flash-attn-2.6.1\nSuccessfully installed flash_attn-2.5.8\nRequirement already satisfied: torch==2.3.1 in /opt/conda/lib/python3.10/site-packages (2.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (3.13.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (2024.5.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (12.1.105)\nRequirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (2.3.1)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1) (12.5.82)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.3.1) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.3.1) (1.3.0)\nRequirement already satisfied: accelerate==0.31.0 in /opt/conda/lib/python3.10/site-packages (0.31.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.31.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.31.0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.31.0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.31.0) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.31.0) (2.3.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate==0.31.0) (0.23.4)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.31.0) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.31.0) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.31.0) (3.13.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.31.0) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.31.0) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.31.0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.31.0) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.31.0) (2024.5.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.31.0) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.31.0) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.31.0) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.31.0) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.31.0) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.31.0) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.31.0) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.31.0) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.31.0) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.31.0) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.31.0) (12.1.105)\nRequirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.31.0) (2.3.1)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.31.0) (12.5.82)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.31.0) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.31.0) (4.66.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.31.0) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.31.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.31.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.31.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.31.0) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.31.0) (1.3.0)\nRequirement already satisfied: transformers==4.41.2 in /opt/conda/lib/python3.10/site-packages (4.41.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2) (0.23.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (2024.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.41.2) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.41.2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.41.2) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.41.2) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.41.2) (2024.7.4)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.20.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.23.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nRequirement already satisfied: trl in /opt/conda/lib/python3.10/site-packages (0.9.6)\nRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl) (2.3.1)\nRequirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from trl) (4.41.2)\nRequirement already satisfied: numpy<2.0.0,>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl) (1.26.4)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl) (0.31.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl) (2.20.0)\nRequirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl) (0.8.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.13.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (2024.5.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (12.1.105)\nRequirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (2.3.1)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->trl) (12.5.82)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.23.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (4.66.4)\nRequirement already satisfied: docstring-parser>=0.16 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (0.16)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (13.7.0)\nRequirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (1.7.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl) (5.9.3)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.9.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.31.0->trl) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (2024.7.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.17.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\nRequirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.11.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.3.1)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.41.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.31.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.23.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.5.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\nRequirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.3.1)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.5.82)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: auto-gptq in /opt/conda/lib/python3.10/site-packages (0.7.1)\nRequirement already satisfied: accelerate>=0.26.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.31.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (2.20.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.2.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (1.26.4)\nRequirement already satisfied: rouge in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (1.0.1)\nRequirement already satisfied: gekko in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (1.2.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (2.3.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.4.3)\nRequirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (4.41.2)\nRequirement already satisfied: peft>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.11.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (4.66.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (0.23.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.13.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (2024.5.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\nRequirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (2.3.1)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->auto-gptq) (12.5.82)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (0.19.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (3.9.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge->auto-gptq) (1.16.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate>=0.26.0->auto-gptq) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (2024.7.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2023.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\nRequirement already satisfied: optimum in /opt/conda/lib/python3.10/site-packages (1.21.2)\nRequirement already satisfied: coloredlogs in /opt/conda/lib/python3.10/site-packages (from optimum) (15.0.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum) (1.13.0)\nRequirement already satisfied: transformers<4.43.0,>=4.26.0 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (4.41.2)\nRequirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.10/site-packages (from optimum) (2.3.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from optimum) (21.3)\nRequirement already satisfied: numpy<2.0 in /opt/conda/lib/python3.10/site-packages (from optimum) (1.26.4)\nRequirement already satisfied: huggingface-hub>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from optimum) (0.23.4)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from optimum) (2.20.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2024.5.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->optimum) (3.1.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.1.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\nRequirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (2.3.1)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->optimum) (12.5.82)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.4.3)\nRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.2.0)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (3.20.3)\nRequirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.10/site-packages (from coloredlogs->optimum) (10.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (3.9.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2024.7.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum) (1.16.0)\nRequirement already satisfied: xformers in /opt/conda/lib/python3.10/site-packages (0.0.27)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from xformers) (1.26.4)\nRequirement already satisfied: torch==2.3.1 in /opt/conda/lib/python3.10/site-packages (from xformers) (2.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->xformers) (3.13.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->xformers) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->xformers) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->xformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->xformers) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->xformers) (2024.5.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->xformers) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->xformers) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->xformers) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->xformers) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->xformers) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->xformers) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->xformers) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->xformers) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->xformers) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->xformers) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->xformers) (12.1.105)\nRequirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->xformers) (2.3.1)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1->xformers) (12.5.82)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.3.1->xformers) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.3.1->xformers) (1.3.0)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.23.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.5.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.7.4)\nCollecting git+https://github.com/microsoft/LoRA\n  Cloning https://github.com/microsoft/LoRA to /tmp/pip-req-build-uwj8iwpk\n  Running command git clone --filter=blob:none --quiet https://github.com/microsoft/LoRA /tmp/pip-req-build-uwj8iwpk\n  Resolved https://github.com/microsoft/LoRA to commit 4c0333854cb905966f8cc4e9a74068c1e507c7b7\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25h","output_type":"stream"}]},{"cell_type":"markdown","source":"> **Packages successfully installed**","metadata":{}},{"cell_type":"markdown","source":"# Model used: microsoft/Phi-3-mini-4k-instruct\nTodo: bnb NF4 configs ``!Bitsandbytes NF4``","metadata":{}},{"cell_type":"code","source":"#load tokens\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")","metadata":{"execution":{"iopub.status.busy":"2024-07-18T09:07:25.413321Z","iopub.execute_input":"2024-07-18T09:07:25.413605Z","iopub.status.idle":"2024-07-18T09:07:25.894076Z","shell.execute_reply.started":"2024-07-18T09:07:25.413575Z","shell.execute_reply":"2024-07-18T09:07:25.893275Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#logging into Hugging Face\n!huggingface-cli login --token $hf_token","metadata":{"execution":{"iopub.status.busy":"2024-07-18T09:07:25.895195Z","iopub.execute_input":"2024-07-18T09:07:25.895546Z","iopub.status.idle":"2024-07-18T09:07:27.551710Z","shell.execute_reply.started":"2024-07-18T09:07:25.895514Z","shell.execute_reply":"2024-07-18T09:07:27.550692Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"from random import randrange\n\nimport torch\nfrom datasets import load_dataset\n\nfrom peft import LoraConfig, prepare_model_for_kbit_training, PeftModel\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    set_seed,\n    pipeline\n)\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2024-07-18T09:07:27.555785Z","iopub.execute_input":"2024-07-18T09:07:27.556101Z","iopub.status.idle":"2024-07-18T09:07:36.383173Z","shell.execute_reply.started":"2024-07-18T09:07:27.556074Z","shell.execute_reply":"2024-07-18T09:07:36.382157Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-07-18 09:07:32.979898: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-18 09:07:32.979968: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-18 09:07:32.981590: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# MODEL_ID is a string that specifies the identifier of the pre-trained model that will be fine-tuned. \n# In this case, the model is 'Phi-3-mini-4k-instruct' from Microsoft.\nMODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n\n# NEW_MODEL_NAME is a string that specifies the name of the new model after fine-tuning.\n# Here, the new model will be named 'opus-samantha-phi-3-mini-4k'.\nNEW_MODEL_NAME = \"sql-xp-phi-3-mini-4k\"","metadata":{"execution":{"iopub.status.busy":"2024-07-18T09:07:36.384615Z","iopub.execute_input":"2024-07-18T09:07:36.385415Z","iopub.status.idle":"2024-07-18T09:07:36.390722Z","shell.execute_reply.started":"2024-07-18T09:07:36.385370Z","shell.execute_reply":"2024-07-18T09:07:36.389636Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# preparing datasets\n\n# DATASET_NAME is a string that specifies the name of the dataset to be used for fine-tuning.\n# Replace \"replace with your dataset\" with the actual name of your dataset.\nDATASET_NAME = synthetic_text_to_sql_dataset_name = \"gretelai/synthetic_text_to_sql\"\n\n# SPLIT specifies the portion of the dataset to be used. In this case, the 'train' split of the dataset will be used.\nSPLIT = \"train\"\n\n# Load the dataset specified by DATASET_NAME using the load_dataset function.\n# The 'split=\"train\"' argument specifies that we want to load the training split of the dataset.\ndataset = load_dataset(DATASET_NAME)\n\ndataset\n\n# Extract relevant fields\n\n# old\n# def extract_fields_synthetic(example):\n#     return {\n#         \"question\": example[\"sql_prompt\"],\n#         \"context\": example[\"sql_context\"],\n#         \"sql\": example[\"sql\"]\n#     }\n# new\ndef extract_fields_synthetic(example):\n    return {\n        \"instruction\": example[\"sql_prompt\"],\n        \"input\": example[\"sql_context\"],\n        \"output\": example[\"sql\"]\n    }\nsynthetic_extracted_dataset = dataset.map(extract_fields_synthetic, remove_columns=dataset['train'].column_names)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-18T10:14:40.844138Z","iopub.execute_input":"2024-07-18T10:14:40.844817Z","iopub.status.idle":"2024-07-18T10:14:43.422635Z","shell.execute_reply.started":"2024-07-18T10:14:40.844785Z","shell.execute_reply":"2024-07-18T10:14:43.421830Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"import random \n\nsynthetic_extracted_train_dataset = synthetic_extracted_dataset[\"train\"]\nsynthetic_extracted_test_dataset = synthetic_extracted_dataset[\"test\"]\n\n# Shuffle the dataset\nsynthetic_extracted_dataset = synthetic_extracted_dataset.shuffle(seed=random.randint(10,99))\nsynthetic_extracted_dataset = synthetic_extracted_dataset.shuffle(seed=random.randint(10,99))\n\nprint(synthetic_extracted_train_dataset)\nprint(synthetic_extracted_test_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-07-18T10:14:43.424361Z","iopub.execute_input":"2024-07-18T10:14:43.424655Z","iopub.status.idle":"2024-07-18T10:14:43.541714Z","shell.execute_reply.started":"2024-07-18T10:14:43.424628Z","shell.execute_reply":"2024-07-18T10:14:43.540813Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['instruction', 'input', 'output'],\n    num_rows: 100000\n})\nDataset({\n    features: ['instruction', 'input', 'output'],\n    num_rows: 5851\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"%whos","metadata":{"execution":{"iopub.status.busy":"2024-07-18T10:14:43.542766Z","iopub.execute_input":"2024-07-18T10:14:43.543088Z","iopub.status.idle":"2024-07-18T10:14:43.572024Z","shell.execute_reply.started":"2024-07-18T10:14:43.543061Z","shell.execute_reply":"2024-07-18T10:14:43.571034Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"Variable                             Type                      Data/Info\n------------------------------------------------------------------------\nAutoModelForCausalLM                 type                      <class 'transformers.mode<...>to.AutoModelForCausalLM'>\nAutoTokenizer                        type                      <class 'transformers.mode<...>tion_auto.AutoTokenizer'>\nBitsAndBytesConfig                   type                      <class 'transformers.util<...>nfig.BitsAndBytesConfig'>\nDATASET_NAME                         str                       gretelai/synthetic_text_to_sql\nLoraConfig                           type                      <class 'peft.tuners.lora.config.LoraConfig'>\nMAX_SEQ_LENGTH                       int                       2048\nMODEL_ID                             str                       microsoft/Phi-3-mini-4k-instruct\nNEW_MODEL_NAME                       str                       sql-xp-phi-3-mini-4k\nPeftModel                            type                      <class 'peft.peft_model.PeftModel'>\nSFTTrainer                           type                      <class 'trl.trainer.sft_trainer.SFTTrainer'>\nSPLIT                                str                       train\nTrainingArguments                    type                      <class 'transformers.trai<...>_args.TrainingArguments'>\nUserSecretsClient                    type                      <class 'kaggle_secrets.UserSecretsClient'>\nattn_implementation                  str                       eager\nbnb_4bit_compute_dtype               str                       bfloat16\nbnb_4bit_quant_type                  str                       nf4\nbnb_config                           BitsAndBytesConfig        BitsAndBytesConfig {\\n  \"<...>hod\": \"bitsandbytes\"\\n}\\n\ncompute_dtype                        dtype                     torch.bfloat16\ncontext_phrase                       str                       \\nCREATE TABLE tasks (\\n <...>EFERENCESusers(id)\\n);\\n\ncontext_phrases                      list                      n=10\ncreate_message_column                function                  <function create_message_<...>column at 0x7a368a519240>\ndataset                              DatasetDict               DatasetDict({\\n    train:<...>um_rows: 5851\\n    })\\n})\ndevice_map                           dict                      n=1\nextract_fields_synthetic             function                  <function extract_fields_<...>thetic at 0x7a36a0aefb50>\nformat_dataset_chatml                function                  <function format_dataset_<...>chatml at 0x7a3674322440>\ngenerated_text                       str                       SELECT AVG(task_count) FR<...>rid) as user_task_counts;\ngradient_accumulation_steps          int                       1\nhf_model_repo                        str                       spectrewolf8/sql-xp-phi-3-mini-4k\nhf_token                             str                       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\ni                                    int                       9\ninput_phrase                         str                       \\ninsert 5 values\\n\ninput_phrases                        list                      n=10\nlearning_rate                        float                     1.41e-05\nlicense                              str                       apache-2.0\nload_dataset                         function                  <function load_dataset at 0x7a3730310c10>\nlora_alpha                           int                       16\nlora_dropout                         float                     0.05\nlora_r                               int                       16\nmodel                                Phi3ForCausalLM           Phi3ForCausalLM(\\n  (mode<...>res=32064, bias=False)\\n)\nnum_train_epochs                     int                       1\noutput                               list                      n=1\noutputs                              list                      n=1\npeft_config                          LoraConfig                LoraConfig(peft_type=<Pef<...>, layer_replication=None)\nper_device_train_batch_size          int                       4\npipe                                 TextGenerationPipeline    <transformers.pipelines.t<...>object at 0x7a35e7a09f00>\npipeline                             function                  <function pipeline at 0x7a36a0c04670>\nprepare_model_for_kbit_training      function                  <function prepare_model_f<...>aining at 0x7a372be979a0>\nprompt                               str                       <|user|>\\n\\n \\ninsert 5 v<...>n<|end|>\\n<|assistant|>\\n\nprompts                              list                      n=10\nrandom                               module                    <module 'random' from '/o<...>ib/python3.10/random.py'>\nrandrange                            method                    <bound method Random.rand<...>bject at 0x5b00a21ab030>>\nrun                                  Run                       <wandb.sdk.wandb_run.Run <...>object at 0x7a3670460370>\nset_seed                             function                  <function set_seed at 0x7a37289571c0>\nsynthetic_extracted_dataset          DatasetDict               DatasetDict({\\n    train:<...>um_rows: 5851\\n    })\\n})\nsynthetic_extracted_test_dataset     Dataset                   Dataset({\\n    features: <...>,\\n    num_rows: 5851\\n})\nsynthetic_extracted_train_dataset    Dataset                   Dataset({\\n    features: <...>n    num_rows: 100000\\n})\nsynthetic_text_to_sql_dataset_name   str                       gretelai/synthetic_text_to_sql\ntarget_modules                       list                      n=7\ntokenizer                            LlamaTokenizerFast        LlamaTokenizerFast(name_o<...>=False, special=True),\\n}\ntorch                                module                    <module 'torch' from '/op<...>kages/torch/__init__.py'>\ntrainer                              SFTTrainer                <trl.trainer.sft_trainer.<...>object at 0x7a368b8a74f0>\ntraining_arguments                   TrainingArguments         TrainingArguments(\\n_n_gp<...>,\\nweight_decay=0.001,\\n)\nuse_4bit                             bool                      True\nuse_double_quant                     bool                      True\nuser_secrets                         UserSecretsClient         <kaggle_secrets.UserSecre<...>object at 0x7a37f4337820>\nusername                             str                       spectrewolf8\nwandb                                module                    <module 'wandb' from '/op<...>kages/wandb/__init__.py'>\nwandb_token                          str                       c3dc335f6704205e4461bbb533e48345c013f8c5\n","output_type":"stream"}]},{"cell_type":"code","source":"# 'set_seed(1234)' sets the random seed for reproducibility.\nset_seed(1234)\n\n# MAX_SEQ_LENGTH is an integer that specifies the maximum length of the sequences that the model will handle.\nMAX_SEQ_LENGTH = 2048\n\n# num_train_epochs is an integer that specifies the number of times the training process will go through the entire dataset.\nnum_train_epochs = 1\n\n# license is a string that specifies the license under which the model is distributed. In this case, it's Apache License 2.0.\nlicense = \"apache-2.0\"\n\n# username is a string that specifies the GitHub username of the person who is fine-tuning the model.\nusername = \"spectrewolf8\"\n\n# learning_rate is a float that specifies the learning rate to be used during training.\nlearning_rate = 1.41e-5\n\n# per_device_train_batch_size is an integer that specifies the number of samples to work through before updating the internal model parameters.\nper_device_train_batch_size = 4\n\n# gradient_accumulation_steps is an integer that specifies the number of steps to accumulate gradients before performing a backward/update pass.\ngradient_accumulation_steps = 1","metadata":{"execution":{"iopub.status.busy":"2024-07-18T10:14:43.575061Z","iopub.execute_input":"2024-07-18T10:14:43.575361Z","iopub.status.idle":"2024-07-18T10:14:43.583657Z","shell.execute_reply.started":"2024-07-18T10:14:43.575336Z","shell.execute_reply":"2024-07-18T10:14:43.582642Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"# 'torch' is a library for scientific computing that provides a wide range of functionalities for dealing with tensors, which are multi-dimensional arrays.\n\n# 'torch.cuda.is_bf16_supported()' is a function that checks if BF16 is supported on the current GPU. BF16 is a data type that uses 16 bits, like float16, but allocates more bits to the exponent, which can result in higher precision.\n\n# 'compute_dtype' is a variable that will hold the data type to be used for computations.\n\n# 'attn_implementation' is a variable that will hold the type of attention implementation to be used.\n\n# 'if torch.cuda.is_bf16_supported():' checks if BF16 is supported on the current GPU. If it is, the following block of code is executed.\n\n# 'compute_dtype = torch.bfloat16' sets 'compute_dtype' to 'torch.bfloat16', which is the BF16 data type in PyTorch.\n\n# 'attn_implementation = 'flash_attention_2'' sets 'attn_implementation' to 'flash_attention_2', which is a type of attention implementation.\n\n# 'else:' specifies that the following block of code should be executed if BF16 is not supported on the current GPU.\n\n# 'compute_dtype = torch.float16' sets 'compute_dtype' to 'torch.float16', which is the float16 data type in PyTorch.\n\n# 'attn_implementation = 'sdpa'' sets 'attn_implementation' to 'sdpa', which is a type of attention implementation.\n\n# 'print(attn_implementation)' prints the value of 'attn_implementation', which is the type of attention implementation to be used.\n\n# 'print(compute_dtype)' prints the value of 'compute_dtype', which is the data type to be used for computations.\nif torch.cuda.is_bf16_supported():\n  compute_dtype = torch.bfloat16\n  attn_implementation = 'flash_attention_2'\nelse:\n  compute_dtype = torch.float16\n  attn_implementation = 'sdpa'\n\nattn_implementation = 'eager'\nprint(attn_implementation)\nprint(compute_dtype)","metadata":{"execution":{"iopub.status.busy":"2024-07-18T10:14:43.584915Z","iopub.execute_input":"2024-07-18T10:14:43.585349Z","iopub.status.idle":"2024-07-18T10:14:43.600315Z","shell.execute_reply.started":"2024-07-18T10:14:43.585317Z","shell.execute_reply":"2024-07-18T10:14:43.599374Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"eager\ntorch.bfloat16\n","output_type":"stream"}]},{"cell_type":"code","source":"#load tokenizr to prepare dataset\n\n# 'AutoTokenizer' is a class from the 'transformers' library that provides a generic tokenizer class from which all other tokenizer classes inherit.\n\n# 'from_pretrained' is a method of the 'AutoTokenizer' class that loads a tokenizer from the Hugging Face Model Hub.\n\n# 'tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)' loads the tokenizer associated with 'tokenizer_id' from the Hugging Face Model Hub and assigns it to the variable 'tokenizer'.\n\n# 'tokenizer.padding_side' is a property of the 'tokenizer' object that determines on which side of the input sequences padding should be added. It can be set to either 'left' or 'right'.\n\n# 'tokenizer.padding_side = 'right'' sets 'tokenizer.padding_side' to 'right', which means that padding will be added to the right side of the input sequences. This is done to prevent warnings that can occur when 'tokenizer.padding_side' is set to 'left'.\n\n# Load the tokenizer associated with the pre-trained model specified by MODEL_ID using the AutoTokenizer class.\n# The 'trust_remote_code=True' argument allows the execution of code from the model card (if any).\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\ntokenizer.padding_side = 'right' # to prevent warnings","metadata":{"execution":{"iopub.status.busy":"2024-07-18T10:14:43.601370Z","iopub.execute_input":"2024-07-18T10:14:43.601669Z","iopub.status.idle":"2024-07-18T10:14:43.881705Z","shell.execute_reply.started":"2024-07-18T10:14:43.601646Z","shell.execute_reply":"2024-07-18T10:14:43.880723Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"def create_message_column(row):\n    messages = []\n    user = {\n        \"content\": f\"{row['instruction']}\\n Input: {row['input']}\",\n        \"role\": \"user\"\n    }\n    messages.append(user)\n    assistant = {\n        \"content\": f\"{row['output']}\",\n        \"role\": \"assistant\"\n    }\n    messages.append(assistant)\n    return {\"messages\": messages}\n\ndef format_dataset_chatml(row):\n    return {\"text\": tokenizer.apply_chat_template(row[\"messages\"], add_generation_prompt=False, tokenize=False)}","metadata":{"execution":{"iopub.status.busy":"2024-07-18T10:14:43.882847Z","iopub.execute_input":"2024-07-18T10:14:43.883152Z","iopub.status.idle":"2024-07-18T10:14:43.889519Z","shell.execute_reply.started":"2024-07-18T10:14:43.883126Z","shell.execute_reply":"2024-07-18T10:14:43.888619Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"# Apply create_message_column function\nsynthetic_extracted_train_dataset = synthetic_extracted_train_dataset.map(create_message_column)\nsynthetic_extracted_test_dataset = synthetic_extracted_test_dataset.map(create_message_column)\n\n# Format dataset using ChatML\nsynthetic_extracted_train_dataset = synthetic_extracted_train_dataset.map(format_dataset_chatml)\nsynthetic_extracted_test_dataset = synthetic_extracted_test_dataset.map(format_dataset_chatml)\n\n# Output the results to verify\nprint(synthetic_extracted_train_dataset)\nprint(synthetic_extracted_test_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-07-18T10:14:43.890945Z","iopub.execute_input":"2024-07-18T10:14:43.891320Z","iopub.status.idle":"2024-07-18T10:14:43.983527Z","shell.execute_reply.started":"2024-07-18T10:14:43.891280Z","shell.execute_reply":"2024-07-18T10:14:43.982597Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['instruction', 'input', 'output', 'messages', 'text'],\n    num_rows: 100000\n})\nDataset({\n    features: ['instruction', 'input', 'output', 'messages', 'text'],\n    num_rows: 5851\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"synthetic_extracted_train_dataset = synthetic_extracted_train_dataset.select(range(10000))\nsynthetic_extracted_test_dataset = synthetic_extracted_test_dataset.select(range(2500))","metadata":{"execution":{"iopub.status.busy":"2024-07-18T10:14:43.984722Z","iopub.execute_input":"2024-07-18T10:14:43.985112Z","iopub.status.idle":"2024-07-18T10:14:43.996800Z","shell.execute_reply.started":"2024-07-18T10:14:43.985077Z","shell.execute_reply":"2024-07-18T10:14:43.995928Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"# !pip install bitsandbytes-cuda110 bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-07-18T10:14:44.001787Z","iopub.execute_input":"2024-07-18T10:14:44.002078Z","iopub.status.idle":"2024-07-18T10:14:44.006326Z","shell.execute_reply.started":"2024-07-18T10:14:44.002054Z","shell.execute_reply":"2024-07-18T10:14:44.005416Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"# 'hf_model_repo' is the identifier for the Hugging Face repository where you want to save the fine-tuned model.\nhf_model_repo=\"spectrewolf8/\"+NEW_MODEL_NAME\n\n# Load Model on GPU \n\n# 'device_map' is a dictionary that maps devices to model parts. In this case, it is set to {\"\": 0}, which means that the entire model will be loaded on GPU 0.\ndevice_map = {\"\": 0}\n\n# Bits and Bytes configuration for the model\n\n# 'use_4bit' is a boolean that controls whether 4-bit precision should be used for loading the base model.\nuse_4bit = True\n\n# 'bnb_4bit_compute_dtype' is the data type that should be used for computations with the 4-bit base model. In this case, it is set to 'bfloat16'.\nbnb_4bit_compute_dtype = \"bfloat16\"\n\n# 'bnb_4bit_quant_type' is the type of quantization that should be used for the 4-bit base model. In this case, it is set to 'nf4'.\nbnb_4bit_quant_type = \"nf4\"\n\n# 'use_double_quant' is a boolean that controls whether nested quantization should be used for the 4-bit base model.\nuse_double_quant = True\n\n# LoRA configuration for the model\n\n# 'lora_r' is the dimension of the LoRA attention.\nlora_r = 16\n\n# 'lora_alpha' is the alpha parameter for LoRA scaling.\nlora_alpha = 16\n\n# 'lora_dropout' is the dropout probability for LoRA layers.\nlora_dropout = 0.05\n\n# 'target_modules' is a list of the modules that should be targeted by LoRA.\ntarget_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"]\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-18T10:14:44.007657Z","iopub.execute_input":"2024-07-18T10:14:44.008015Z","iopub.status.idle":"2024-07-18T10:14:44.017022Z","shell.execute_reply.started":"2024-07-18T10:14:44.007968Z","shell.execute_reply":"2024-07-18T10:14:44.016293Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=use_4bit,\n        bnb_4bit_quant_type=bnb_4bit_quant_type,\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=use_double_quant,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-18T10:14:44.018219Z","iopub.execute_input":"2024-07-18T10:14:44.018507Z","iopub.status.idle":"2024-07-18T10:14:44.031906Z","shell.execute_reply.started":"2024-07-18T10:14:44.018477Z","shell.execute_reply":"2024-07-18T10:14:44.031017Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"# 'AutoTokenizer' is a class from the Hugging Face Transformers library that provides a tokenizer for a given pre-trained model.\n\n# 'from_pretrained' is a method of the 'AutoTokenizer' class that loads a tokenizer from a pre-trained model.\n\n# 'trust_remote_code=True' is a parameter that allows the execution of remote code when loading the tokenizer.\n\n# 'add_eos_token=True' is a parameter that adds an end-of-sentence token to the tokenizer.\n\n# 'use_fast=True' is a parameter that uses the fast version of the tokenizer, if available.\n\n# 'tokenizer.pad_token = tokenizer.unk_token' sets the padding token of the tokenizer to be the same as the unknown token.\n\n# 'tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)' sets the ID of the padding token to be the same as the ID of the padding token.\n\n# 'tokenizer.padding_side = 'left'' sets the side where padding will be added to be the left side.\n\n# 'BitsAndBytesConfig' is a class that provides a configuration for quantization.\n\n# 'bnb_config' is a variable that holds the configuration for quantization.\n\n# 'AutoModelForCausalLM' is a class from the Hugging Face Transformers library that provides a model for causal language modeling.\n\n# 'from_pretrained' is a method of the 'AutoModelForCausalLM' class that loads a model from a pre-trained model.\n\n# 'torch_dtype=compute_dtype' is a parameter that sets the data type of the model to be the same as 'compute_dtype'.\n\n# 'quantization_config=bnb_config' is a parameter that sets the configuration for quantization to be 'bnb_config'.\n\n# 'device_map=device_map' is a parameter that sets the device map of the model to be 'device_map'.\n\n# 'attn_implementation=attn_implementation' is a parameter that sets the type of attention implementation to be 'attn_implementation'.\n\n# 'prepare_model_for_kbit_training' is a function that prepares a model for k-bit training.\n\n# 'model = prepare_model_for_kbit_training(model)' prepares 'model' for k-bit training and assigns the result back to 'model'.\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True, add_eos_token=True, use_fast=True)\ntokenizer.pad_token = tokenizer.unk_token\ntokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\ntokenizer.padding_side = 'left'\n\nmodel = AutoModelForCausalLM.from_pretrained(\n          MODEL_ID, torch_dtype=compute_dtype, trust_remote_code=True, quantization_config=bnb_config, device_map=device_map,\n          attn_implementation=attn_implementation\n)\n\nmodel = prepare_model_for_kbit_training(model)\nmodel.gradient_checkpointing_enable()","metadata":{"execution":{"iopub.status.busy":"2024-07-18T10:14:44.033167Z","iopub.execute_input":"2024-07-18T10:14:44.033444Z","iopub.status.idle":"2024-07-18T10:14:49.727868Z","shell.execute_reply.started":"2024-07-18T10:14:44.033421Z","shell.execute_reply":"2024-07-18T10:14:49.727092Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdcc57418a474cdc8a0b3e58335926d2"}},"metadata":{}}]},{"cell_type":"code","source":"# 'wandb' is a library for machine learning experiment tracking, dataset versioning, and model management.\n\n# 'import wandb' is a line of code that imports the 'wandb' library.\n\n# 'wandb.login()' is a function that logs you into your Weights & Biases account. If you're not logged in, it will prompt you to enter your API key.\n\n# This block of code is used to initialize Weights & Biases for experiment tracking.\n\n# get wandb token\nwandb_token = user_secrets.get_secret(\"WANDB_TOKEN\")\n\nimport wandb\nwandb.login(key = wandb_token)\n\nrun = wandb.init(\n    project='Training and tuning Phi-3-mini-4k-instruct for SQL | kaggle-sql-xp-phi-3-mini-4k-instruct.ipynb', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-18T10:14:49.728961Z","iopub.execute_input":"2024-07-18T10:14:49.729228Z","iopub.status.idle":"2024-07-18T10:15:14.610957Z","shell.execute_reply.started":"2024-07-18T10:14:49.729206Z","shell.execute_reply":"2024-07-18T10:15:14.609789Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:1h4gahu2) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td></td></tr><tr><td>train/global_step</td><td></td></tr><tr><td>train/grad_norm</td><td></td></tr><tr><td>train/learning_rate</td><td></td></tr><tr><td>train/loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>3917708208783360.0</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/grad_norm</td><td>0.3427</td></tr><tr><td>train/learning_rate</td><td>0.0002</td></tr><tr><td>train/loss</td><td>0.6112</td></tr><tr><td>train_loss</td><td>0.52477</td></tr><tr><td>train_runtime</td><td>753.0591</td></tr><tr><td>train_samples_per_second</td><td>1.328</td></tr><tr><td>train_steps_per_second</td><td>0.332</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">worthy-durian-3</strong> at: <a href='https://wandb.ai/spectrewolf8-cui/Training%20and%20tuning%20Phi-3-mini-4k-instruct%20for%20SQL%20%7C%20kaggle-sql-xp-phi-3-mini-4k-instruct.ipynb/runs/1h4gahu2' target=\"_blank\">https://wandb.ai/spectrewolf8-cui/Training%20and%20tuning%20Phi-3-mini-4k-instruct%20for%20SQL%20%7C%20kaggle-sql-xp-phi-3-mini-4k-instruct.ipynb/runs/1h4gahu2</a><br/> View project at: <a href='https://wandb.ai/spectrewolf8-cui/Training%20and%20tuning%20Phi-3-mini-4k-instruct%20for%20SQL%20%7C%20kaggle-sql-xp-phi-3-mini-4k-instruct.ipynb' target=\"_blank\">https://wandb.ai/spectrewolf8-cui/Training%20and%20tuning%20Phi-3-mini-4k-instruct%20for%20SQL%20%7C%20kaggle-sql-xp-phi-3-mini-4k-instruct.ipynb</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240718_091059-1h4gahu2/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:1h4gahu2). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240718_101450-grtt1sce</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/spectrewolf8-cui/Training%20and%20tuning%20Phi-3-mini-4k-instruct%20for%20SQL%20%7C%20kaggle-sql-xp-phi-3-mini-4k-instruct.ipynb/runs/grtt1sce' target=\"_blank\">frosty-surf-4</a></strong> to <a href='https://wandb.ai/spectrewolf8-cui/Training%20and%20tuning%20Phi-3-mini-4k-instruct%20for%20SQL%20%7C%20kaggle-sql-xp-phi-3-mini-4k-instruct.ipynb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/spectrewolf8-cui/Training%20and%20tuning%20Phi-3-mini-4k-instruct%20for%20SQL%20%7C%20kaggle-sql-xp-phi-3-mini-4k-instruct.ipynb' target=\"_blank\">https://wandb.ai/spectrewolf8-cui/Training%20and%20tuning%20Phi-3-mini-4k-instruct%20for%20SQL%20%7C%20kaggle-sql-xp-phi-3-mini-4k-instruct.ipynb</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/spectrewolf8-cui/Training%20and%20tuning%20Phi-3-mini-4k-instruct%20for%20SQL%20%7C%20kaggle-sql-xp-phi-3-mini-4k-instruct.ipynb/runs/grtt1sce' target=\"_blank\">https://wandb.ai/spectrewolf8-cui/Training%20and%20tuning%20Phi-3-mini-4k-instruct%20for%20SQL%20%7C%20kaggle-sql-xp-phi-3-mini-4k-instruct.ipynb/runs/grtt1sce</a>"},"metadata":{}}]},{"cell_type":"code","source":"#Hyperparamter\ntraining_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    save_steps=25,\n    logging_steps=10,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n    report_to=\"wandb\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-18T10:15:14.614876Z","iopub.execute_input":"2024-07-18T10:15:14.615635Z","iopub.status.idle":"2024-07-18T10:15:14.654350Z","shell.execute_reply.started":"2024-07-18T10:15:14.615587Z","shell.execute_reply":"2024-07-18T10:15:14.653270Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"# peft config\npeft_config = LoraConfig(\n    lora_alpha = lora_alpha,\n    lora_dropout = lora_dropout,\n    r = lora_r,\n    target_modules=target_modules\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-18T10:15:14.656179Z","iopub.execute_input":"2024-07-18T10:15:14.656568Z","iopub.status.idle":"2024-07-18T10:15:14.662825Z","shell.execute_reply.started":"2024-07-18T10:15:14.656532Z","shell.execute_reply":"2024-07-18T10:15:14.661832Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"# 'SFTTrainer' is a class that provides a trainer for fine-tuning a model.\n\n# 'trainer' is a variable that holds the trainer.\n\n# 'model=model' is a parameter that sets the model to be trained to be 'model'.\n\n# 'train_dataset=dataset_chatml['train']' is a parameter that sets the training dataset to be 'dataset_chatml['train']'.\n\n# 'eval_dataset=dataset_chatml['test']' is a parameter that sets the evaluation dataset to be 'dataset_chatml['test']'.\n\n# 'peft_config=peft_config' is a parameter that sets the configuration for the Lora layer to be 'peft_config'.\n\n# 'dataset_text_field=\"text\"' is a parameter that sets the field in the dataset that contains the text to be 'text'.\n\n# 'max_seq_length=512' is a parameter that sets the maximum sequence length for the model to be 512.\n\n# 'tokenizer=tokenizer' is a parameter that sets the tokenizer to be 'tokenizer'.\n\n# 'args=args' is a parameter that sets the training arguments to be 'args'.\n\n# This line of code is used to create a trainer for fine-tuning the model with the specified parameters.\ntrainer = SFTTrainer(\n        model=model,\n        train_dataset=synthetic_extracted_train_dataset,\n        eval_dataset=synthetic_extracted_test_dataset,\n        peft_config=peft_config,\n        dataset_text_field=\"text\",\n        max_seq_length=512,\n        tokenizer=tokenizer,\n        args=training_arguments,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-18T10:15:14.664113Z","iopub.execute_input":"2024-07-18T10:15:14.664438Z","iopub.status.idle":"2024-07-18T10:15:17.755484Z","shell.execute_reply.started":"2024-07-18T10:15:14.664406Z","shell.execute_reply":"2024-07-18T10:15:17.754559Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71dd3c7d675c43438d3b76d9d8a28bed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4da73d4794a43bfb99a9b55e1c2db03"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:408: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# !pip install -U flash_attn","metadata":{"execution":{"iopub.status.busy":"2024-07-18T10:15:17.756617Z","iopub.execute_input":"2024-07-18T10:15:17.756913Z","iopub.status.idle":"2024-07-18T10:15:30.516749Z","shell.execute_reply.started":"2024-07-18T10:15:17.756886Z","shell.execute_reply":"2024-07-18T10:15:30.515487Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: flash_attn in /opt/conda/lib/python3.10/site-packages (2.6.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash_attn) (2.3.1)\nRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from flash_attn) (0.8.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn) (3.13.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn) (2024.5.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn) (12.1.105)\nRequirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn) (2.3.1)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash_attn) (12.5.82)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->flash_attn) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash_attn) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# 'trainer.train()' is a method that starts the training of the model. It uses the training dataset, model, and training arguments that were specified when the trainer was created.\n\n# 'trainer.save_model()' is a method that saves the trained model to the local file system. The model will be saved in the output directory that was specified in the training arguments.\n\n# This block of code is used to train the model and then save the trained model to the local file system.\n# train\ntrainer.train()\n\n# save model in local\ntrainer.save_model()","metadata":{"execution":{"iopub.status.busy":"2024-07-18T10:15:30.518753Z","iopub.execute_input":"2024-07-18T10:15:30.519176Z","iopub.status.idle":"2024-07-18T12:21:28.588578Z","shell.execute_reply.started":"2024-07-18T10:15:30.519137Z","shell.execute_reply":"2024-07-18T12:21:28.587432Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2500/2500 2:05:46, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.738700</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.620100</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.534000</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.591200</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.634500</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.419200</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.472700</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.534100</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.540700</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.601600</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.403600</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.440500</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.495100</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.514200</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.655700</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.408700</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.438400</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.499200</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.521200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.628600</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.400400</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.517900</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.465900</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.560700</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.653700</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.433400</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.450600</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.447100</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.525700</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.636300</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.403900</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.442400</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.457400</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.497800</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.617800</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.396700</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.429500</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.478800</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.494800</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.610900</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.409300</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.421800</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.474900</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.517600</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.630000</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.408800</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.427700</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.437000</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.534300</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.629800</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.391700</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.421900</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.461400</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.516400</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.602200</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.419300</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.439800</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.492400</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.500300</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.600100</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.423200</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.424100</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.448300</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.505300</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.607700</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.426500</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.430600</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.455100</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.491400</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.591500</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.373700</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.441600</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.461100</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.533100</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.587600</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.406100</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.417300</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.466000</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.513000</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.597500</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.412000</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.421800</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.451700</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.496300</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.586900</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.391300</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.438200</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.438700</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.512100</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.580700</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.400700</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.447800</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.470300</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.481100</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.604500</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.388400</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.414900</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.503200</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.508000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.591000</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>0.377400</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.431900</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>0.463500</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.479900</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.566100</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.375300</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>0.427300</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.469900</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>0.505200</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.575300</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>0.376800</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.455000</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>0.436600</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.501300</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.563600</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.357900</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>0.424300</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.464200</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>0.519300</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.556200</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>0.395000</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.421200</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>0.436200</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.461200</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.568600</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.403900</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>0.420100</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.430500</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.621900</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>0.390100</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.433000</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>0.425200</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.468800</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.580000</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.386200</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>0.400400</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.454800</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>0.454000</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.585000</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>0.382400</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.414700</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>0.436700</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.497900</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.624400</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.368600</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>0.429500</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.435100</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>0.462300</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.563000</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>0.375900</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.429500</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>0.434900</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.457200</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.574400</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>0.383200</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>0.423600</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>0.447100</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>0.478300</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.587400</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>0.357900</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>0.425900</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>0.439500</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>0.465600</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.560200</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>0.366600</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>0.431300</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>0.460300</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>0.482500</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.593200</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>0.383900</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>0.402700</td>\n    </tr>\n    <tr>\n      <td>1730</td>\n      <td>0.438500</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>0.463300</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.524000</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>0.371800</td>\n    </tr>\n    <tr>\n      <td>1770</td>\n      <td>0.413200</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>0.484600</td>\n    </tr>\n    <tr>\n      <td>1790</td>\n      <td>0.466300</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.553100</td>\n    </tr>\n    <tr>\n      <td>1810</td>\n      <td>0.367800</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>0.415500</td>\n    </tr>\n    <tr>\n      <td>1830</td>\n      <td>0.436400</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>0.492900</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.560500</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>0.351600</td>\n    </tr>\n    <tr>\n      <td>1870</td>\n      <td>0.393700</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>0.447400</td>\n    </tr>\n    <tr>\n      <td>1890</td>\n      <td>0.490800</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.573600</td>\n    </tr>\n    <tr>\n      <td>1910</td>\n      <td>0.362400</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>0.428300</td>\n    </tr>\n    <tr>\n      <td>1930</td>\n      <td>0.422500</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>0.481900</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.553100</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>0.361600</td>\n    </tr>\n    <tr>\n      <td>1970</td>\n      <td>0.405800</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>0.425700</td>\n    </tr>\n    <tr>\n      <td>1990</td>\n      <td>0.483600</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.516700</td>\n    </tr>\n    <tr>\n      <td>2010</td>\n      <td>0.366500</td>\n    </tr>\n    <tr>\n      <td>2020</td>\n      <td>0.397500</td>\n    </tr>\n    <tr>\n      <td>2030</td>\n      <td>0.462200</td>\n    </tr>\n    <tr>\n      <td>2040</td>\n      <td>0.492200</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>0.552000</td>\n    </tr>\n    <tr>\n      <td>2060</td>\n      <td>0.368300</td>\n    </tr>\n    <tr>\n      <td>2070</td>\n      <td>0.409200</td>\n    </tr>\n    <tr>\n      <td>2080</td>\n      <td>0.402200</td>\n    </tr>\n    <tr>\n      <td>2090</td>\n      <td>0.488900</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.574500</td>\n    </tr>\n    <tr>\n      <td>2110</td>\n      <td>0.360900</td>\n    </tr>\n    <tr>\n      <td>2120</td>\n      <td>0.427100</td>\n    </tr>\n    <tr>\n      <td>2130</td>\n      <td>0.417300</td>\n    </tr>\n    <tr>\n      <td>2140</td>\n      <td>0.486200</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>0.596200</td>\n    </tr>\n    <tr>\n      <td>2160</td>\n      <td>0.354700</td>\n    </tr>\n    <tr>\n      <td>2170</td>\n      <td>0.429400</td>\n    </tr>\n    <tr>\n      <td>2180</td>\n      <td>0.417000</td>\n    </tr>\n    <tr>\n      <td>2190</td>\n      <td>0.481800</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.586000</td>\n    </tr>\n    <tr>\n      <td>2210</td>\n      <td>0.371500</td>\n    </tr>\n    <tr>\n      <td>2220</td>\n      <td>0.408400</td>\n    </tr>\n    <tr>\n      <td>2230</td>\n      <td>0.450100</td>\n    </tr>\n    <tr>\n      <td>2240</td>\n      <td>0.454200</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>0.602500</td>\n    </tr>\n    <tr>\n      <td>2260</td>\n      <td>0.337600</td>\n    </tr>\n    <tr>\n      <td>2270</td>\n      <td>0.428800</td>\n    </tr>\n    <tr>\n      <td>2280</td>\n      <td>0.427500</td>\n    </tr>\n    <tr>\n      <td>2290</td>\n      <td>0.472000</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.584700</td>\n    </tr>\n    <tr>\n      <td>2310</td>\n      <td>0.357900</td>\n    </tr>\n    <tr>\n      <td>2320</td>\n      <td>0.397900</td>\n    </tr>\n    <tr>\n      <td>2330</td>\n      <td>0.453500</td>\n    </tr>\n    <tr>\n      <td>2340</td>\n      <td>0.430400</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>0.568400</td>\n    </tr>\n    <tr>\n      <td>2360</td>\n      <td>0.351000</td>\n    </tr>\n    <tr>\n      <td>2370</td>\n      <td>0.423700</td>\n    </tr>\n    <tr>\n      <td>2380</td>\n      <td>0.441400</td>\n    </tr>\n    <tr>\n      <td>2390</td>\n      <td>0.497700</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.585900</td>\n    </tr>\n    <tr>\n      <td>2410</td>\n      <td>0.360300</td>\n    </tr>\n    <tr>\n      <td>2420</td>\n      <td>0.396100</td>\n    </tr>\n    <tr>\n      <td>2430</td>\n      <td>0.433800</td>\n    </tr>\n    <tr>\n      <td>2440</td>\n      <td>0.488100</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>0.584800</td>\n    </tr>\n    <tr>\n      <td>2460</td>\n      <td>0.353200</td>\n    </tr>\n    <tr>\n      <td>2470</td>\n      <td>0.395500</td>\n    </tr>\n    <tr>\n      <td>2480</td>\n      <td>0.451700</td>\n    </tr>\n    <tr>\n      <td>2490</td>\n      <td>0.456900</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.510800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"hf_model_repo= \"spectrewolf8/sql-xp-phi-3-mini-4k\"\ntrainer.push_to_hub(hf_model_repo)\ntrainer.model.push_to_hub(hf_model_repo)\ntokenizer.push_to_hub(hf_model_repo)","metadata":{"execution":{"iopub.status.busy":"2024-07-18T12:21:28.590068Z","iopub.execute_input":"2024-07-18T12:21:28.590370Z","iopub.status.idle":"2024-07-18T12:21:48.065223Z","shell.execute_reply.started":"2024-07-18T12:21:28.590342Z","shell.execute_reply":"2024-07-18T12:21:48.064166Z"},"trusted":true},"execution_count":74,"outputs":[{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f0f95bcd3ad4640bf5505c376050d2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/35.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a847ce5299b49ff9882d110123fcb99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/5.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85b3aea1098040bfbe2e42bdb1c2be97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/35.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b99896b5eda4d74a1f632f01a9c1fa9"}},"metadata":{}},{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/spectrewolf8/sql-xp-phi-3-mini-4k/commit/8498ab9b16bb2dd2127299d043fcfc4e896f9260', commit_message='Upload tokenizer', commit_description='', oid='8498ab9b16bb2dd2127299d043fcfc4e896f9260', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Model stats","metadata":{}},{"cell_type":"code","source":"wandb.finish()\nmodel.config.use_cache = True\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2024-07-18T12:24:46.976473Z","iopub.execute_input":"2024-07-18T12:24:46.976824Z","iopub.status.idle":"2024-07-18T12:24:56.176436Z","shell.execute_reply.started":"2024-07-18T12:24:46.976800Z","shell.execute_reply":"2024-07-18T12:24:56.175489Z"},"trusted":true},"execution_count":82,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td></td></tr><tr><td>train/global_step</td><td></td></tr><tr><td>train/grad_norm</td><td></td></tr><tr><td>train/learning_rate</td><td></td></tr><tr><td>train/loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>3.938091109810176e+16</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>2500</td></tr><tr><td>train/grad_norm</td><td>0.35459</td></tr><tr><td>train/learning_rate</td><td>0.0002</td></tr><tr><td>train/loss</td><td>0.5108</td></tr><tr><td>train_loss</td><td>0.47145</td></tr><tr><td>train_runtime</td><td>7554.136</td></tr><tr><td>train_samples_per_second</td><td>1.324</td></tr><tr><td>train_steps_per_second</td><td>0.331</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">frosty-surf-4</strong> at: <a href='https://wandb.ai/spectrewolf8-cui/Training%20and%20tuning%20Phi-3-mini-4k-instruct%20for%20SQL%20%7C%20kaggle-sql-xp-phi-3-mini-4k-instruct.ipynb/runs/grtt1sce' target=\"_blank\">https://wandb.ai/spectrewolf8-cui/Training%20and%20tuning%20Phi-3-mini-4k-instruct%20for%20SQL%20%7C%20kaggle-sql-xp-phi-3-mini-4k-instruct.ipynb/runs/grtt1sce</a><br/> View project at: <a href='https://wandb.ai/spectrewolf8-cui/Training%20and%20tuning%20Phi-3-mini-4k-instruct%20for%20SQL%20%7C%20kaggle-sql-xp-phi-3-mini-4k-instruct.ipynb' target=\"_blank\">https://wandb.ai/spectrewolf8-cui/Training%20and%20tuning%20Phi-3-mini-4k-instruct%20for%20SQL%20%7C%20kaggle-sql-xp-phi-3-mini-4k-instruct.ipynb</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240718_101450-grtt1sce/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."},"metadata":{}},{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"Phi3ForCausalLM(\n  (model): Phi3Model(\n    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n    (embed_dropout): Dropout(p=0.0, inplace=False)\n    (layers): ModuleList(\n      (0-31): 32 x Phi3DecoderLayer(\n        (self_attn): Phi3Attention(\n          (o_proj): lora.Linear(\n            (base_layer): Linear(in_features=3072, out_features=3072, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Dropout(p=0.05, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=3072, out_features=16, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=16, out_features=3072, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n          )\n          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n          (rotary_emb): Phi3RotaryEmbedding()\n        )\n        (mlp): Phi3MLP(\n          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n          (down_proj): lora.Linear(\n            (base_layer): Linear(in_features=8192, out_features=3072, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Dropout(p=0.05, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=8192, out_features=16, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=16, out_features=3072, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n          )\n          (activation_fn): SiLU()\n        )\n        (input_layernorm): Phi3RMSNorm()\n        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n        (post_attention_layernorm): Phi3RMSNorm()\n      )\n    )\n    (norm): Phi3RMSNorm()\n  )\n  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Testing model","metadata":{}},{"cell_type":"code","source":"synthetic_extracted_test_dataset[10]","metadata":{"execution":{"iopub.status.busy":"2024-07-18T12:21:48.066479Z","iopub.execute_input":"2024-07-18T12:21:48.066878Z","iopub.status.idle":"2024-07-18T12:21:48.086581Z","shell.execute_reply.started":"2024-07-18T12:21:48.066845Z","shell.execute_reply":"2024-07-18T12:21:48.085533Z"},"trusted":true},"execution_count":75,"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"{'instruction': \"How many decentralized applications have been downloaded from the 'Asia-Pacific' region?\",\n 'input': \"CREATE TABLE dapp_ranking (dapp_id INT, dapp_name VARCHAR(50), dapp_category VARCHAR(30), dapp_rating DECIMAL(3,2), dapp_downloads INT, dapp_region VARCHAR(30)); INSERT INTO dapp_ranking (dapp_id, dapp_name, dapp_category, dapp_rating, dapp_downloads, dapp_region) VALUES (1, 'AsiaPacificDapp', 'Social', 4.3, 2000000, 'Asia-Pacific');\",\n 'output': \"SELECT SUM(dapp_downloads) FROM dapp_ranking WHERE dapp_region = 'Asia-Pacific';\",\n 'messages': [{'content': \"How many decentralized applications have been downloaded from the 'Asia-Pacific' region?\\n Input: CREATE TABLE dapp_ranking (dapp_id INT, dapp_name VARCHAR(50), dapp_category VARCHAR(30), dapp_rating DECIMAL(3,2), dapp_downloads INT, dapp_region VARCHAR(30)); INSERT INTO dapp_ranking (dapp_id, dapp_name, dapp_category, dapp_rating, dapp_downloads, dapp_region) VALUES (1, 'AsiaPacificDapp', 'Social', 4.3, 2000000, 'Asia-Pacific');\",\n   'role': 'user'},\n  {'content': \"SELECT SUM(dapp_downloads) FROM dapp_ranking WHERE dapp_region = 'Asia-Pacific';\",\n   'role': 'assistant'}],\n 'text': \"<|user|>\\nHow many decentralized applications have been downloaded from the 'Asia-Pacific' region?\\n Input: CREATE TABLE dapp_ranking (dapp_id INT, dapp_name VARCHAR(50), dapp_category VARCHAR(30), dapp_rating DECIMAL(3,2), dapp_downloads INT, dapp_region VARCHAR(30)); INSERT INTO dapp_ranking (dapp_id, dapp_name, dapp_category, dapp_rating, dapp_downloads, dapp_region) VALUES (1, 'AsiaPacificDapp', 'Social', 4.3, 2000000, 'Asia-Pacific');<|end|>\\n<|assistant|>\\nSELECT SUM(dapp_downloads) FROM dapp_ranking WHERE dapp_region = 'Asia-Pacific';<|end|>\\n<|endoftext|>\"}"},"metadata":{}}]},{"cell_type":"code","source":"pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-07-18T12:21:48.087858Z","iopub.execute_input":"2024-07-18T12:21:48.088137Z","iopub.status.idle":"2024-07-18T12:21:48.146468Z","shell.execute_reply.started":"2024-07-18T12:21:48.088114Z","shell.execute_reply":"2024-07-18T12:21:48.145341Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"input_phrase = \"\"\"\ninsert 5 values\n\"\"\"\ncontext_phrase = \"\"\"\nCREATE TABLE tasks (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    task_name VARCHAR(100) NOT NULL,\n    userid INT NOT NULL,\n    date DATE NOT NULL,\n    FOREIGN KEY (userid) REFERENCESusers(id)\n);\n\"\"\"\nprompt = pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": f\"\\n {input_phrase} Input:{context_phrase}\"}], tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, num_beams=1, temperature=0.3, top_k=50, top_p=0.95, max_time= 180)\nprint(outputs[0]['generated_text'][len(prompt):].strip())","metadata":{"execution":{"iopub.status.busy":"2024-07-18T12:21:48.147700Z","iopub.execute_input":"2024-07-18T12:21:48.148005Z","iopub.status.idle":"2024-07-18T12:22:44.213332Z","shell.execute_reply.started":"2024-07-18T12:21:48.147959Z","shell.execute_reply":"2024-07-18T12:22:44.212421Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"INSERT INTO tasks (name, task_name, userid, date) VALUES ('John', 'Task 1', 1, '2022-01-01'), ('Jane', 'Task 2', 2, '2022-01-02'), ('Bob', 'Task 3', 3, '2022-01-03'), ('Alice', 'Task 4', 4, '2022-01-04'), ('Charlie', 'Task 5', 5, '2022-01-05');\n","output_type":"stream"}]},{"cell_type":"code","source":"input_phrases = [\n    \"insert 5 values\",\n    \"select all records\",\n    \"update record with id 3\",\n    \"delete all records where task_name is 'coding'\",\n    \"add a new column 'status' to the table\",\n    \"find all tasks with userid 2\",\n    \"count the number of tasks per user\",\n    \"list all tasks sorted by date\",\n    \"join tasks with users\",\n    \"find the average number of tasks per user\"\n]\n\ncontext_phrases = [\n    \"\"\"\n    CREATE TABLE tasks (\n        id INT AUTO_INCREMENT PRIMARY KEY,\n        name VARCHAR(100) NOT NULL,\n        task_name VARCHAR(100) NOT NULL,\n        userid INT NOT NULL,\n        date DATE NOT NULL,\n        FOREIGN KEY (userid) REFERENCES users(id)\n    );\n    \"\"\",\n    \"\"\"\n    CREATE TABLE tasks (\n        id INT AUTO_INCREMENT PRIMARY KEY,\n        name VARCHAR(100) NOT NULL,\n        task_name VARCHAR(100) NOT NULL,\n        userid INT NOT NULL,\n        date DATE NOT NULL,\n        FOREIGN KEY (userid) REFERENCES users(id)\n    );\n    \"\"\",\n    \"\"\"\n    CREATE TABLE tasks (\n        id INT AUTO_INCREMENT PRIMARY KEY,\n        name VARCHAR(100) NOT NULL,\n        task_name VARCHAR(100) NOT NULL,\n        userid INT NOT NULL,\n        date DATE NOT NULL,\n        FOREIGN KEY (userid) REFERENCES users(id)\n    );\n    \"\"\",\n    \"\"\"\n    CREATE TABLE tasks (\n        id INT AUTO_INCREMENT PRIMARY KEY,\n        name VARCHAR(100) NOT NULL,\n        task_name VARCHAR(100) NOT NULL,\n        userid INT NOT NULL,\n        date DATE NOT NULL,\n        FOREIGN KEY (userid) REFERENCES users(id)\n    );\n    \"\"\",\n    \"\"\"\n    CREATE TABLE tasks (\n        id INT AUTO_INCREMENT PRIMARY KEY,\n        name VARCHAR(100) NOT NULL,\n        task_name VARCHAR(100) NOT NULL,\n        userid INT NOT NULL,\n        date DATE NOT NULL,\n        FOREIGN KEY (userid) REFERENCES users(id)\n    );\n    \"\"\",\n    \"\"\"\n    CREATE TABLE tasks (\n        id INT AUTO_INCREMENT PRIMARY KEY,\n        name VARCHAR(100) NOT NULL,\n        task_name VARCHAR(100) NOT NULL,\n        userid INT NOT NULL,\n        date DATE NOT NULL,\n        FOREIGN KEY (userid) REFERENCES users(id)\n    );\n    \"\"\",\n    \"\"\"\n    CREATE TABLE tasks (\n        id INT AUTO_INCREMENT PRIMARY KEY,\n        name VARCHAR(100) NOT NULL,\n        task_name VARCHAR(100) NOT NULL,\n        userid INT NOT NULL,\n        date DATE NOT NULL,\n        FOREIGN KEY (userid) REFERENCES users(id)\n    );\n    \"\"\",\n    \"\"\"\n    CREATE TABLE tasks (\n        id INT AUTO_INCREMENT PRIMARY KEY,\n        name VARCHAR(100) NOT NULL,\n        task_name VARCHAR(100) NOT NULL,\n        userid INT NOT NULL,\n        date DATE NOT NULL,\n        FOREIGN KEY (userid) REFERENCES users(id)\n    );\n    \"\"\",\n    \"\"\"\n    CREATE TABLE tasks (\n        id INT AUTO_INCREMENT PRIMARY KEY,\n        name VARCHAR(100) NOT NULL,\n        task_name VARCHAR(100) NOT NULL,\n        userid INT NOT NULL,\n        date DATE NOT NULL,\n        FOREIGN KEY (userid) REFERENCES users(id)\n    );\n    \"\"\",\n    \"\"\"\n    CREATE TABLE tasks (\n        id INT AUTO_INCREMENT PRIMARY KEY,\n        name VARCHAR(100) NOT NULL,\n        task_name VARCHAR(100) NOT NULL,\n        userid INT NOT NULL,\n        date DATE NOT NULL,\n        FOREIGN KEY (userid) REFERENCES users(id)\n    );\n    \"\"\"\n]\n\n# Apply the chat template to create prompts\nprompts = [pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": f\"\\n {input_phrase} Input:{context_phrase}\"}], tokenize=False, add_generation_prompt=True)\n           for input_phrase, context_phrase in zip(input_phrases, context_phrases)]\n\n# Generate SQL queries\noutputs = [pipe(prompt, max_new_tokens=256, do_sample=True, num_beams=1, temperature=0.3, top_k=50, top_p=0.95, max_time=180)\n           for prompt in prompts]\n\n# Print the results\nfor i, output in enumerate(outputs):\n    generated_text = output[0]['generated_text'][len(prompts[i]):].strip()\n    print(f\"Prompt {i+1}:\")\n    print(generated_text)\n    print(\"\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-18T12:22:44.214916Z","iopub.execute_input":"2024-07-18T12:22:44.215313Z","iopub.status.idle":"2024-07-18T12:24:35.498918Z","shell.execute_reply.started":"2024-07-18T12:22:44.215278Z","shell.execute_reply":"2024-07-18T12:24:35.497801Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stdout","text":"Prompt 1:\nINSERT INTO tasks (name, task_name, userid, date) VALUES (1, 'Task 1', 1, '2022-01-01'), (2, 'Task 2', 2, '2022-01-02'), (3, 'Task 3', 3, '2022-01-03'), (4, 'Task 4', 4, '2022-01-04'), (5, 'Task 5', 5, '2022-01-05');\n\n\nPrompt 2:\nSELECT * FROM tasks;\n\n\nPrompt 3:\nUPDATE tasks SET task_name = 'Updated Task Name' WHERE id = 3;\n\n\nPrompt 4:\nDELETE FROM tasks WHERE task_name = 'coding';\n\n\nPrompt 5:\nALTER TABLE tasks ADD COLUMN status VARCHAR(20) NOT NULL DEFAULT 'pending';\n\n\nPrompt 6:\nSELECT * FROM tasks WHERE userid = 2;\n\n\nPrompt 7:\nSELECT userid, COUNT(*) as num_tasks FROM tasks GROUP BY userid;\n\n\nPrompt 8:\nSELECT * FROM tasks ORDER BY date;\n\n\nPrompt 9:\nSELECT t.name, t.task_name, u.name, u.email FROM tasks t JOIN users u ON t.userid = u.id;\n\n\nPrompt 10:\nSELECT AVG(task_count) FROM (SELECT userid, COUNT(*) as task_count FROM tasks GROUP BY userid) as subquery;\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Loading model from hugging face","metadata":{}},{"cell_type":"code","source":"\n\n# 'hf_model_repo' is a variable that holds the repository name for the Hugging Face model.\n\n# This line of code is used to reference the repository name for the Hugging Face model.\n\n# 'hf_model_repo' is a variable that holds the repository name for the Hugging Face model.\n\n# 'username/modelname' is the repository name, where 'username' is the username of the repository owner and 'modelname' is the name of the model.\n\n# This line of code is used to set the repository name for the Hugging Face model.\nhf_model_repo= \"spectrewolf8/sql-xp-phi-3-mini-4k\"\n     \n\n# Retrieve the model and tokenizer from the Hub.\n\n# 'device_map' is a variable that holds the mapping of the devices that are used for computation.\n\n# 'compute_dtype' is a variable that holds the data type that is used for computation.\n\n# This line of code is used to return the values of the 'device_map' and 'compute_dtype' variables.\ndevice_map, compute_dtype\n     \n\n# This block of code is used to import the necessary libraries, set the seed for reproducibility, and load a pre-trained tokenizer and model.\n\n# 'import torch' is a line of code that imports the PyTorch library, which is a popular open-source machine learning library.\n\n# 'from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed' is a line of code that imports the 'AutoTokenizer', 'AutoModelForCausalLM', and 'set_seed' functions from the Hugging Face Transformers library.\n\n# 'set_seed(1234)' is a line of code that sets the seed for the random number generator to '1234'. This is done to ensure that the results are reproducible.\n\n# 'tokenizer = AutoTokenizer.from_pretrained(hf_model_repo,trust_remote_code=True)' is a line of code that loads a pre-trained tokenizer from the Hugging Face Model Hub. 'hf_model_repo' is the repository name for the model and 'trust_remote_code=True' allows the execution of code from the model file.\n\n# 'model = AutoModelForCausalLM.from_pretrained(hf_model_repo, trust_remote_code=True, torch_dtype=compute_dtype, device_map=device_map)' is a line of code that loads a pre-trained model for causal language modeling from the Hugging Face Model Hub. 'hf_model_repo' is the repository name for the model, 'trust_remote_code=True' allows the execution of code from the model file, 'torch_dtype=compute_dtype' sets the data type for the PyTorch tensors, and 'device_map=device_map' sets the device mapping.\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n\nset_seed(1234)  # For reproducibility\n\ntokenizer = AutoTokenizer.from_pretrained(hf_model_repo,trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(hf_model_repo, trust_remote_code=True, torch_dtype=compute_dtype, device_map=device_map) # compute \"auto\" dev_map \"cuda\"","metadata":{"execution":{"iopub.status.busy":"2024-07-18T12:24:35.500528Z","iopub.execute_input":"2024-07-18T12:24:35.500903Z","iopub.status.idle":"2024-07-18T12:24:45.765872Z","shell.execute_reply.started":"2024-07-18T12:24:35.500868Z","shell.execute_reply":"2024-07-18T12:24:45.764812Z"},"trusted":true},"execution_count":79,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.85M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce93e743d51c4f64b8e81d875726d59f"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5572bbb0daea4f2e93b0dc1b023b5bf6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/35.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"800e4a48fa334c3d8e2803161f99f5ca"}},"metadata":{}}]},{"cell_type":"code","source":"pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-07-18T12:24:45.767239Z","iopub.execute_input":"2024-07-18T12:24:45.768359Z","iopub.status.idle":"2024-07-18T12:24:45.774589Z","shell.execute_reply.started":"2024-07-18T12:24:45.768319Z","shell.execute_reply":"2024-07-18T12:24:45.773726Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"input_phrase = \"\"\"\ninsert 5 values\n\"\"\"\ncontext_phrase = \"\"\"\nCREATE TABLE tasks (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    task_name VARCHAR(100) NOT NULL,\n    userid INT NOT NULL,\n    date DATE NOT NULL,\n    FOREIGN KEY (userid) REFERENCESusers(id)\n);\n\"\"\"\nprompt = pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": f\"\\n {input_phrase} Input:{context_phrase}\"}], tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, num_beams=1, temperature=0.3, top_k=50, top_p=0.95, max_time= 180)\nprint(outputs[0]['generated_text'][len(prompt):].strip())","metadata":{"execution":{"iopub.status.busy":"2024-07-18T12:25:42.475423Z","iopub.execute_input":"2024-07-18T12:25:42.475822Z","iopub.status.idle":"2024-07-18T12:25:50.598344Z","shell.execute_reply.started":"2024-07-18T12:25:42.475794Z","shell.execute_reply":"2024-07-18T12:25:50.597332Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stdout","text":"INSERT INTO tasks (name, task_name, userid, date) VALUES \n    ('Alice', 'Task 1', 1, '2022-01-01'),\n    ('Bob', 'Task 2', 2, '2022-01-02'),\n    ('Charlie', 'Task 3', 3, '2022-01-03'),\n    ('David', 'Task 4', 4, '2022-01-04'),\n    ('Eve', 'Task 5', 5, '2022-01-05');\n","output_type":"stream"}]}]}