{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install flash_attn==2.5.8\n!pip install torch==2.3.1\n!pip install accelerate==0.31.0\n!pip install transformers==4.41.2\n!pip install datasets\n!pip install transformers\n!pip install trl\n!pip install peft \n!pip install auto-gptq \n!pip install optimum\n!pip install xformers\n!pip install huggingface_hub\n!pip install git+https://github.com/microsoft/LoRA\n    \n#bits and bytes with cuda\n!pip install bitsandbytes-cuda110 bitsandbytes","metadata":{"_uuid":"6a8624f6-55b1-4e10-ab3c-4cff980d7bef","_cell_guid":"45bcc8ae-b977-43de-b3fd-ad567abfb018","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-07-20T16:58:16.606934Z","iopub.execute_input":"2024-07-20T16:58:16.607969Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Collecting flash_attn==2.5.8\n  Downloading flash_attn-2.5.8.tar.gz (2.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash_attn==2.5.8) (2.1.2)\nCollecting einops (from flash_attn==2.5.8)\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from flash_attn==2.5.8) (21.3)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from flash_attn==2.5.8) (1.11.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->flash_attn==2.5.8) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn==2.5.8) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn==2.5.8) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn==2.5.8) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn==2.5.8) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn==2.5.8) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn==2.5.8) (2024.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->flash_attn==2.5.8) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash_attn==2.5.8) (1.3.0)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: flash_attn\n  Building wheel for flash_attn (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for flash_attn: filename=flash_attn-2.5.8-cp310-cp310-linux_x86_64.whl size=120607435 sha256=b962f0eb38a2e54a3ece20b4b43f59f5a638ce53fe6e269992c58b119425d1f0\n  Stored in directory: /root/.cache/pip/wheels/9b/5b/2b/dea8af4e954161c49ef1941938afcd91bb93689371ed12a226\nSuccessfully built flash_attn\nInstalling collected packages: einops, flash_attn\nSuccessfully installed einops-0.8.0 flash_attn-2.5.8\nCollecting torch==2.3.1\n  Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (3.13.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (2024.5.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.1)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.1)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.1)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.1)\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.1)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.1)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.1)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.1)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.1)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.1)\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.1)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.3.1 (from torch==2.3.1)\n  Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1)\n  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.3.1) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.3.1) (1.3.0)\nDownloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m176.0/176.2 MB\u001b[0m \u001b[31m160.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m00:01\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"### Important: You must restart the kernel at this point after installing the packages!!\n---","metadata":{}},{"cell_type":"markdown","source":"# Preparing datasets, loading model and tokenizer, Training model \n## Model used: microsoft/Phi-3-mini-4k-instruct","metadata":{"_uuid":"71117f00-9b26-47e5-a5d9-3599eac29d86","_cell_guid":"4c8b0c17-ac6f-4bd0-b986-6ba7fd2f6022","trusted":true}},{"cell_type":"code","source":"#load tokens\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\n\n#logging into Hugging Face\n!huggingface-cli login --token $hf_token","metadata":{"_uuid":"95d58080-d53e-4eda-87e7-b7ee14291058","_cell_guid":"df14167b-04ed-4a09-b32c-d3317529faf7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# impoting classes\nfrom random import randrange\n\nimport torch\nfrom datasets import load_dataset\n\nfrom peft import LoraConfig, prepare_model_for_kbit_training, PeftModel\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    set_seed,\n    pipeline\n)\nfrom trl import SFTTrainer","metadata":{"_uuid":"23de7275-89df-4171-bf8a-18babedad5ad","_cell_guid":"a9dd1bf4-d90e-4bd2-9172-4f1384735570","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preparing datasets\n\n# DATASET_NAME is a string that specifies the name of the dataset to be used for fine-tuning.\nDATASET_NAME = synthetic_text_to_sql_dataset_name = \"gretelai/synthetic_text_to_sql\"\n\n# Load the dataset specified by DATASET_NAME using the load_dataset function.\ndataset = load_dataset(DATASET_NAME)\n\ndataset\n\n# Extract relevant fields\n\n# old\n# def extract_fields_synthetic(example):\n#     return {\n#         \"question\": example[\"sql_prompt\"],\n#         \"context\": example[\"sql_context\"],\n#         \"sql\": example[\"sql\"]\n#     }\n\n# new\ndef extract_fields_synthetic(example):\n    return {\n        \"instruction\": example[\"sql_prompt\"],\n        \"input\": example[\"sql_context\"],\n        \"output\": example[\"sql\"]\n    }\nsynthetic_extracted_dataset = dataset.map(extract_fields_synthetic, remove_columns=dataset['train'].column_names)","metadata":{"_uuid":"681d7ce8-ac37-4748-9ded-54c32aa46125","_cell_guid":"d7f45229-1ee3-447f-aee7-555e2e69a605","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split and shuffle datasets\n\nimport random \n\nsynthetic_extracted_train_dataset = synthetic_extracted_dataset[\"train\"]\nsynthetic_extracted_test_dataset = synthetic_extracted_dataset[\"test\"]\n\n# Shuffle the dataset\nsynthetic_extracted_dataset = synthetic_extracted_dataset.shuffle(seed=random.randint(10,99))\nsynthetic_extracted_dataset = synthetic_extracted_dataset.shuffle(seed=random.randint(10,99))\n\nprint(synthetic_extracted_train_dataset)\nprint(synthetic_extracted_test_dataset)","metadata":{"_uuid":"2043c70e-ac11-490e-bf5f-679a4f96004f","_cell_guid":"0ab25899-16dd-4535-8d18-92d688566e71","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'torch.cuda.is_bf16_supported()' is a function that checks if BF16 is supported on the current GPU. BF16 is a data type that uses 16 bits, like float16, but allocates more bits to the exponent, which can result in higher precision.\n# 'attn_implementation' is a variable that will hold the type of attention implementation to be used.\n\nif torch.cuda.is_bf16_supported():\n  compute_dtype = torch.bfloat16\nelse:\n  compute_dtype = torch.float16\n\nattn_implementation = 'eager'\nprint(attn_implementation)\nprint(compute_dtype)","metadata":{"_uuid":"27cf4526-590c-4548-818b-49c4e5dc0e28","_cell_guid":"7adeb82c-f155-4111-9989-c2fe2f09c8da","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load tokenizr to prepare dataset\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\ntokenizer.padding_side = 'right' # to prevent warnings","metadata":{"_uuid":"e6898036-74e9-4de8-902b-91b5580cd98e","_cell_guid":"90bbf293-9511-4880-be8a-8d8da57e5692","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Define message/prompt creation and formatting methods for the datasets\n# #prompt will have our prompt/instruction\n# #context will have out SQL context i.e table creation sql command\n\ndef create_message_column(row):\n    message = []\n    user = {\n        \"content\": f\"\\n #prompt: {row['instruction']}\\n #context: {row['input']}\",\n        \"role\": \"user\"\n    }\n    message.append(user)\n    assistant = {\n        \"content\": f\"{row['output']}\",\n        \"role\": \"assistant\"\n    }\n    message.append(assistant)\n    return {\"message\": message}\n\ndef format_dataset_with_chat_template(row):\n    return {\"text\": tokenizer.apply_chat_template(row[\"message\"], add_generation_prompt=False, tokenize=False)}","metadata":{"_uuid":"739b1003-5485-45a4-bbd0-139efb6f2355","_cell_guid":"d45bffac-0f90-4ee2-88b5-bfe5d1911cd3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply create_message_column function\nsynthetic_extracted_train_dataset = synthetic_extracted_train_dataset.map(create_message_column)\nsynthetic_extracted_test_dataset = synthetic_extracted_test_dataset.map(create_message_column)\n\n# Format dataset using \nsynthetic_extracted_train_dataset = synthetic_extracted_train_dataset.map(format_dataset_with_chat_template)\nsynthetic_extracted_test_dataset = synthetic_extracted_test_dataset.map(format_dataset_with_chat_template)\n\n# Output the results to verify\nprint(synthetic_extracted_train_dataset)\nprint(synthetic_extracted_test_dataset)","metadata":{"_uuid":"9f1ce49c-8473-46b6-b658-68a4212593cf","_cell_guid":"e19fab45-cdad-4307-bfe3-5ba1abcc7520","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#select subsets of datasets\n# 75:25 dataset ratio\n\nsynthetic_extracted_train_dataset = synthetic_extracted_train_dataset.select(range(1000))\nsynthetic_extracted_test_dataset = synthetic_extracted_test_dataset.select(range(330))","metadata":{"_uuid":"f0c8fff8-d966-408a-95a4-56833a43930a","_cell_guid":"829a96fb-73ed-4f02-b8b6-0f0b26a9da7d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MODEL_ID is a string that specifies the identifier of the pre-trained model that will be fine-tuned. \nMODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n\n# NEW_MODEL_NAME is a string that specifies the name of the new model after fine-tuning.\nNEW_MODEL_NAME = \"sql-xp-phi-3-mini-4k\"","metadata":{"_uuid":"fa206877-4936-4044-ab91-ce4191943a5d","_cell_guid":"18925152-1c77-4661-ab68-b9d322b5f5bd","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'hf_model_repo' is the identifier for the Hugging Face repository where you want to save the fine-tuned model.\nhf_model_repo=\"spectrewolf8/\"+NEW_MODEL_NAME\n\n# Load Model on GPU \n# 'device_map' is set to {\"\": 0}, which means that the entire model will be loaded on GPU 0.\ndevice_map = {\"\": 0}\n\n\n# Bits and Bytes configuration for the model\n\n# 'load_in_4bit' is a boolean that control if 4bit quantization should be loaded. In this case, it is set to True\n# 'bnb_4bit_compute_dtype' is the data type that should be used for computations with the 4-bit base model. In this case, it is set to 'bfloat16'.\n# 'bnb_4bit_quant_type' is the type of quantization that should be used for the 4-bit base model. In this case, it is set to 'nf4'.\n# 'bnb_4bit_use_double_quant' is a boolean that controls whether nested quantization should be used for the 4-bit base model.\n\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=\"bfloat16\",\n        bnb_4bit_use_double_quant=True,\n)\n\n# LoRA configuration for the model\n\n# 'lora_r' is the dimension of the LoRA attention.\n# 'lora_alpha' is the alpha parameter for LoRA scaling.\n# 'lora_dropout' is the dropout probability for LoRA layers.\n# 'target_modules' is a list of the modules that should be targeted by LoRA.\n\nlora_r = 16\nlora_alpha = 16\nlora_dropout = 0.05\ntarget_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"]\n\n# peft configuration for the model\npeft_config = LoraConfig(\n    lora_alpha = lora_alpha,\n    lora_dropout = lora_dropout,\n    r = lora_r,\n    target_modules=target_modules\n)\n","metadata":{"_uuid":"6871e894-f9cb-471d-8fe3-f886ef779429","_cell_guid":"d10ecbf4-b945-4cd8-9baa-f7f0cdc84fe5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'set_seed(1234)' sets the random seed for reproducibility.\nset_seed(1234)\n\n# username is a string that specifies the GitHub username of the person who is fine-tuning the model.\n# license is a string that specifies the license under which the model is distributed. In this case, it's Apache License 2.0.\n\nusername = \"spectrewolf8\"\nlicense = \"apache-2.0\"\n\n# MAX_SEQ_LENGTH is an integer that specifies the maximum length of the sequences that the model will handle.\n# num_train_epochs is an integer that specifies the number of times the training process will go through the entire dataset.\n# learning_rate is a float that specifies the learning rate to be used during training.\n# per_device_train_batch_size is an integer that specifies the number of samples to work through before updating the internal model parameters.\n# gradient_accumulation_steps is an integer that specifies the number of steps to accumulate gradients before performing a backward/update pass.\n\nMAX_SEQ_LENGTH = 2048\nnum_train_epochs = 1\nlearning_rate = 1.41e-5\nper_device_train_batch_size = 4\ngradient_accumulation_steps = 1","metadata":{"_uuid":"0d92181c-0500-47c0-8715-54132cd5cc4a","_cell_guid":"409d29d7-c1b3-4dd6-b40e-36268b8d1481","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'AutoTokenizer' is a class from the Hugging Face Transformers library that provides a tokenizer for a given pre-trained model.\n# 'from_pretrained' is a method of the 'AutoTokenizer' class that loads a tokenizer from a pre-trained model.\n# 'trust_remote_code=True' is a parameter that allows the execution of remote code when loading the tokenizer.\n# 'add_eos_token=True' is a parameter that adds an end-of-sentence token to the tokenizer.\n# 'use_fast=True' is a parameter that uses the fast version of the tokenizer, if available.\n# 'tokenizer.pad_token = tokenizer.unk_token' sets the padding token of the tokenizer to be the same as the unknown token.\n# 'tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)' sets the ID of the padding token to be the same as the ID of the padding token.\n# 'tokenizer.padding_side = 'left'' sets the side where padding will be added to be the left side.\n# 'BitsAndBytesConfig' is a class that provides a configuration for quantization.\n# 'bnb_config' is a variable that holds the configuration for quantization.\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True, add_eos_token=True, use_fast=True)\ntokenizer.pad_token = tokenizer.unk_token\ntokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\ntokenizer.padding_side = 'left'\n\n# 'AutoModelForCausalLM' is a class from the Hugging Face Transformers library that provides a model for causal language modeling.\n# 'from_pretrained' is a method of the 'AutoModelForCausalLM' class that loads a model from a pre-trained model.\n# 'torch_dtype=compute_dtype' is a parameter that sets the data type of the model to be the same as 'compute_dtype'.\n# 'quantization_config=bnb_config' is a parameter that sets the configuration for quantization to be 'bnb_config'.\n# 'device_map=device_map' is a parameter that sets the device map of the model to be 'device_map'.\n# 'attn_implementation=attn_implementation' is a parameter that sets the type of attention implementation to be 'attn_implementation'.\n# 'model = prepare_model_for_kbit_training(model)' prepares 'model' for k-bit training and assigns the result back to 'model'.\n\nmodel = AutoModelForCausalLM.from_pretrained(\n          MODEL_ID, torch_dtype=compute_dtype, trust_remote_code=True, quantization_config=bnb_config, device_map=device_map,\n          attn_implementation=attn_implementation\n)\n\nmodel = prepare_model_for_kbit_training(model)\nmodel.gradient_checkpointing_enable()","metadata":{"_uuid":"ffa95fb9-f2d4-42d7-87ef-acc7731ef5b9","_cell_guid":"78e0448f-df29-4c92-b5ea-39a5734408d4","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This block of code is used to initialize Weights & Biases (wandb) for experiment tracking.\n\n# Retrieve the Weights & Biases API token from user secrets\nwandb_token = user_secrets.get_secret(\"WANDB_TOKEN\")\n\n# Import the wandb library for experiment tracking\nimport wandb\n\n# Log in to Weights & Biases using the retrieved API token\nwandb.login(key=wandb_token)\n\n# Initialize a new Weights & Biases run for tracking the experiment\nrun = wandb.init(\n    project='Training and tuning Phi-3-mini-4k-instruct for SQL | kaggle-sql-xp-phi-3-mini-4k-instruct.ipynb', \n    job_type=\"training\",  # Specify the type of job as training\n    anonymous=\"allow\"     # Allow anonymous logging if no user is logged in\n)\n","metadata":{"_uuid":"4babda66-818f-4949-aa32-0d48efc8c52e","_cell_guid":"4eed93a1-a3ce-4cc8-8147-6af4dedb366c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'TrainingArguments' is a class from the Hugging Face Transformers library that provides hyperparameters for training.\n# 'output_dir=\"./results\"' sets the directory where the training results (like checkpoints and logs) will be saved.\n# 'num_train_epochs=1' sets the number of times the entire training dataset will be passed through the model.\n# 'per_device_train_batch_size=4' sets the batch size for training on each device (e.g., GPU).\n# 'gradient_accumulation_steps=1' sets the number of steps to accumulate gradients before performing a backward/update pass.\n# 'optim=\"paged_adamw_32bit\"' specifies the optimizer to use; in this case, \"paged_adamw_32bit\" is used.\n# 'save_steps=25' specifies the number of steps before saving a checkpoint.\n# 'logging_steps=10' specifies the number of steps before logging training metrics.\n# 'learning_rate=2e-4' sets the learning rate for the optimizer.\n# 'weight_decay=0.001' applies weight decay (L2 regularization) to prevent overfitting.\n# 'fp16=False' specifies whether to use 16-bit (half-precision) floating point.\n# 'bf16=False' specifies whether to use bfloat16 precision (an alternative to fp16).\n# 'max_grad_norm=0.3' clips the gradient norm to prevent the exploding gradient problem.\n# 'max_steps=-1' specifies the total number of training steps; -1 means no limit.\n# 'warmup_ratio=0.03' sets the proportion of training steps to perform learning rate warmup.\n# 'group_by_length=True' groups sequences of similar lengths together for efficient training.\n# 'lr_scheduler_type=\"constant\"' specifies the type of learning rate scheduler; in this case, it uses a constant learning rate.\n# 'report_to=\"wandb\"' specifies the reporting tool to use for logging; in this case, Weights and Biases (wandb) is used.\n\ntraining_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    save_steps=25,\n    logging_steps=10,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n    report_to=\"wandb\"\n)","metadata":{"_uuid":"39edfcae-d186-4c3c-80d5-79662640e46f","_cell_guid":"d680ca36-2903-4b11-afb6-4b295712178a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'SFTTrainer' is a class that provides a trainer for fine-tuning a model.\n# 'trainer' is a variable that holds the trainer.\n# 'model=model' is a parameter that sets the model to be trained to be 'model'.\n# 'train_dataset=synthetic_extracted_train_dataset' is a parameter that sets the training dataset to be 'synthetic_extracted_train_dataset'.\n# 'eval_dataset=synthetic_extracted_test_dataset' is a parameter that sets the evaluation dataset to be 'synthetic_extracted_test_dataset'.\n# 'peft_config=peft_config' is a parameter that sets the configuration for the Lora layer to be 'peft_config'.\n# 'dataset_text_field=\"text\"' is a parameter that sets the field in the dataset that contains the text to be 'text'.\n# 'max_seq_length=512' is a parameter that sets the maximum sequence length for the model to be 512.\n# 'tokenizer=tokenizer' is a parameter that sets the tokenizer to be 'tokenizer'.\n# 'args=args' is a parameter that sets the training arguments to be 'args'.\n# This line of code is used to create a trainer for fine-tuning the model with the specified parameters.\n\ntrainer = SFTTrainer(\n        model=model,\n        train_dataset=synthetic_extracted_train_dataset,\n        eval_dataset=synthetic_extracted_test_dataset,\n        peft_config=peft_config,\n        dataset_text_field=\"text\",\n        max_seq_length=512,\n        tokenizer=tokenizer,\n        args=training_arguments,\n        callbacks=[CustomWandbCallback],\n)","metadata":{"_uuid":"f265b6da-e6dd-43f6-a3c2-5c52b51a7ae3","_cell_guid":"993077f9-6844-4547-b3b8-94af55f8996a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'trainer.train()' is a method that starts the training of the model. It uses the training dataset, model, and training arguments that were specified when the trainer was created.\n\n# train the model\ntrainer.train()","metadata":{"_uuid":"a81801fa-503c-4372-87a8-bf382990de49","_cell_guid":"c3e749ad-5c69-496b-9fc0-c39a1e00e894","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'trainer.save_model()' is a method that saves the trained model to the local file system. The model will be saved in the output directory that was specified in the training arguments.\n# This block of code is used to train the model and then save the trained model to the local file system.\n\n# save model locally\ntrainer.save_model()\ntokenizer.save_pretrained(\"./results\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainer.save_model(\"./path_to_save_model\")  # Save the model locally to specified directory\n# tokenizer.save_pretrained(\"./path_to_save_model\")  # Save the tokenizer to specified directory","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the repository name on the Hugging Face Hub where the model, trainer, and tokenizer will be pushed.\nhf_model_repo = \"spectrewolf8/sql-xp-phi-3-mini-4k\"\n\n# Push the trainer to the Hugging Face Hub.\n# This includes training arguments, optimizer states, and other relevant information.\ntrainer.push_to_hub(hf_model_repo)\n\n# Push the model to the Hugging Face Hub.\n# This saves the model weights and configuration to the specified repository.\ntrainer.model.push_to_hub(hf_model_repo)\n\n# Push the tokenizer to the Hugging Face Hub.\n# This saves the tokenizer configuration and vocab files to the specified repository.\ntokenizer.push_to_hub(hf_model_repo)","metadata":{"_uuid":"4e8b7157-7244-4c51-b982-ae0cdac63a5f","_cell_guid":"61b97cd0-4f5d-4ed1-8f5a-3ed68630d09f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model stats","metadata":{"_uuid":"446fa004-e371-49de-a38c-6072a87e9db2","_cell_guid":"689a3ca6-0fd0-460c-8076-fa61050bdbc6","trusted":true}},{"cell_type":"code","source":"# Finish the Weights & Biases (wandb) run.\n# This finalizes the current experiment run, ensuring all data is uploaded and the run is properly closed.\nwandb.finish()\n\n# Set the 'use_cache' configuration option of the model to True.\n# This enables caching of the computation results during inference, which can speed up the model's performance.\nmodel.config.use_cache = True\n\n# Set the model to evaluation mode.\n# This changes the model's behavior to inference mode, disabling features like dropout that are only used during training.\nmodel.eval()","metadata":{"_uuid":"ff47bfdc-f5f1-4741-bf0a-e580b336d72f","_cell_guid":"c0c1d925-0709-48c0-be3e-cc98cde700f5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing model","metadata":{"_uuid":"3a3559cb-8f94-44d4-8124-dae7a660b2a4","_cell_guid":"179d450f-226f-44b7-b7f9-3e83115f9e9b","trusted":true}},{"cell_type":"code","source":"# Create a text generation pipeline using the specified model and tokenizer.\n# The 'pipeline' function sets up a ready-to-use text generation pipeline, combining the model and tokenizer.\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)","metadata":{"_uuid":"727a395e-7ba7-44c3-afb2-ce1cc84b5138","_cell_guid":"f0b8bf83-8a2a-46c8-985f-b4dbda88336c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## example-1","metadata":{}},{"cell_type":"code","source":"# Define the input phrase which represents the user's request or query.\ninput_phrase = \"\"\"\ninsert 5 values\n\"\"\"\n\n# Define the context phrase which provides the SQL table schema relevant to the input phrase.\ncontext_phrase = \"\"\"\nCREATE TABLE tasks (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    task_name VARCHAR(100) NOT NULL,\n    userid INT NOT NULL,\n    date DATE NOT NULL,\n    FOREIGN KEY (userid) REFERENCES users(id)\n);\n\"\"\"\n\n# Create a prompt by applying a chat template to the input and context phrases using the tokenizer.\n# The 'apply_chat_template' method formats the input as a chat message, making it suitable for text generation.\n# 'tokenize=False' indicates that the input should not be tokenized yet.\n# 'add_generation_prompt=True' adds a prompt for text generation.\nprompt = pipe.tokenizer.apply_chat_template(\n    [{\"role\": \"user\", \"content\": f\"\\n {input_phrase} Input:{context_phrase}\"}],\n    tokenize=False,\n    add_generation_prompt=True\n)\n\n# Generate text using the pipeline with the specified parameters.\n# 'max_new_tokens=256' sets the maximum number of new tokens to generate.\n# 'do_sample=True' enables sampling for text generation.\n# 'num_beams=1' specifies the number of beams for beam search (1 means no beam search).\n# 'temperature=0.3' controls the randomness of predictions by scaling the logits before applying softmax.\n# 'top_k=50' considers only the top 50 token predictions for sampling.\n# 'top_p=0.95' enables nucleus sampling, considering tokens that have a cumulative probability of 0.95.\n# 'max_time=180' sets the maximum generation time to 180 seconds.\noutputs = pipe(\n    prompt,\n    max_new_tokens=256,\n    do_sample=True,\n    num_beams=1,\n    temperature=0.3,\n    top_k=50,\n    top_p=0.95,\n    max_time=180\n)\n\n# Print the generated text by stripping out the prompt portion and displaying only the new generated content.\nprint(outputs[0]['generated_text'][len(prompt):].strip())","metadata":{"_uuid":"b4e10e3e-08dd-4d2d-b549-d30e399f165a","_cell_guid":"ff546dab-fde6-4fe5-97c6-fd4daaada22b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## example-2","metadata":{}},{"cell_type":"code","source":"# Define a list of input phrases representing various SQL operations.\ninput_phrases = [\n    \"insert 5 values\",\n    \"select all records\",\n    \"update record with id 3\",\n    \"delete all records where task_name is 'coding'\",\n    \"add a new column 'status' to the table\",\n    \"find all tasks with userid 2\",\n    \"count the number of tasks per user\",\n    \"list all tasks sorted by date\",\n    \"join tasks with users\",\n    \"find the average number of tasks per user\"\n]\n\n# Define a list of context phrases which provide the SQL table schema.\n# The same context is used for all input phrases in this example.\ncontext_phrases = [\n    \"\"\"\n    CREATE TABLE tasks (\n        id INT AUTO_INCREMENT PRIMARY KEY,\n        name VARCHAR(100) NOT NULL,\n        task_name VARCHAR(100) NOT NULL,\n        userid INT NOT NULL,\n        date DATE NOT NULL,\n        FOREIGN KEY (userid) REFERENCES users(id)\n    );\n    \"\"\"\n] * len(input_phrases)  # Repeat the same context for all input phrases.\n\n# Apply the chat template to create prompts by combining input and context phrases.\n# The 'apply_chat_template' method formats each input phrase with its corresponding context phrase.\n# f\"\\n #prompt: {input_phrase}\\n #context: {context_phrase}\"\n\nprompts = [pipe.tokenizer.apply_chat_template(\n    [{\"role\": \"user\", \"content\": f\"\\n #prompt: {input_phrase}\\n #context: {context_phrase}\"}], \n    tokenize=False, \n    add_generation_prompt=True\n) for input_phrase, context_phrase in zip(input_phrases, context_phrases)]\n\n# Generate SQL queries using the text generation pipeline with specified parameters.\n# Each prompt is passed through the pipeline to generate the corresponding SQL query.\noutputs = [pipe(\n    prompt, \n    max_new_tokens=256, \n    do_sample=True, \n    num_beams=1, \n    temperature=0.3, \n    top_k=50, \n    top_p=0.95, \n    max_time=180\n) for prompt in prompts]\n\n# Print the results of the generated SQL queries.\n# For each generated output, strip out the prompt portion and display only the new generated content.\nfor i, output in enumerate(outputs):\n    generated_text = output[0]['generated_text'][len(prompts[i]):].strip()\n    print(f\"Prompt {i+1}:\")\n    print(generated_text)\n    print(\"\\n\")","metadata":{"_uuid":"90f64fe8-7659-4ae3-ab39-837333371378","_cell_guid":"3e5601fd-2053-4e04-af29-4b70f8489c1a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading model from hugging face","metadata":{"_uuid":"b6dbcb95-e61b-49d8-b002-1a1db813e41d","_cell_guid":"ecf220b2-da9a-4957-a0e3-a25e50a87c61","trusted":true}},{"cell_type":"code","source":"# Import necessary libraries\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n\n# Set the seed for the random number generator to ensure reproducibility\nset_seed(1234)\n\n# Define the repository name for the Hugging Face model\n# 'hf_model_repo' is a variable that holds the repository name for the Hugging Face model\n# 'username/modelname' is the repository name, where 'username' is the username of the repository owner\n# and 'modelname' is the name of the model\nhf_model_repo = \"spectrewolf8/sql-xp-phi-3-mini-4k\"\n\n# Retrieve the device mapping and computation data type\n# 'device_map' is a variable that holds the mapping of the devices that are used for computation\n# 'compute_dtype' is a variable that holds the data type that is used for computation\n\n# device_map = {\"\": 0}\n# compute_dtype = torch.bfloat16 or torch.float16\ndevice_map, compute_dtype\n\n# Load a pre-trained tokenizer from the Hugging Face Model Hub\n# 'tokenizer' is the variable that holds the tokenizer\n# 'trust_remote_code=True' allows the execution of code from the model file\ntokenizer = AutoTokenizer.from_pretrained(hf_model_repo, trust_remote_code=True)\n\n# Load a pre-trained model for causal language modeling from the Hugging Face Model Hub\n# 'model' is the variable that holds the model\n# 'trust_remote_code=True' allows the execution of code from the model file\n# 'torch_dtype=compute_dtype' sets the data type for the PyTorch tensors\n# 'device_map=device_map' sets the device mapping\nmodel = AutoModelForCausalLM.from_pretrained(hf_model_repo, trust_remote_code=True, torch_dtype=compute_dtype, device_map=device_map)\n","metadata":{"_uuid":"0cbabe22-4da7-4728-8ed8-ec166dcbfa4d","_cell_guid":"ef1c2247-f9d3-4df5-84e9-4b282d8d79c3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)","metadata":{"_uuid":"42b23cf6-f554-40c8-88d4-83bda2be7dfe","_cell_guid":"10d7c594-f27e-4458-a779-e1f3fef77ab3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Rest of the steps from here are the same as example-1 and example-2","metadata":{}},{"cell_type":"code","source":"# Define the context and input phrase\ncontext_phrase = \"\"\"\nCREATE TABLE users (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    username VARCHAR(50) NOT NULL,\n    email VARCHAR(100) NOT NULL,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE TABLE projects (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    project_name VARCHAR(100) NOT NULL,\n    description TEXT,\n    start_date DATE NOT NULL,\n    end_date DATE,\n    user_id INT,\n    FOREIGN KEY (user_id) REFERENCES users(id)\n);\n\nCREATE TABLE tasks (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    task_name VARCHAR(100) NOT NULL,\n    status VARCHAR(20) CHECK (status IN ('pending', 'in_progress', 'completed')),\n    priority INT CHECK (priority BETWEEN 1 AND 5),\n    project_id INT,\n    assigned_to INT,\n    due_date DATE,\n    FOREIGN KEY (project_id) REFERENCES projects(id),\n    FOREIGN KEY (assigned_to) REFERENCES users(id)\n);\n\nCREATE TABLE comments (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    task_id INT,\n    user_id INT,\n    comment_text TEXT NOT NULL,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (task_id) REFERENCES tasks(id),\n    FOREIGN KEY (user_id) REFERENCES users(id)\n);\n\"\"\"\n\ninput_phrase = \"\"\"\nUpdate the status of tasks to 'completed' for all tasks that have passed their due date. Also, update the end date of the corresponding projects to the current date if all tasks in the project are completed.\n\"\"\"\n\n# Apply the chat template to create the prompt\nprompt = pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": f\"\\n #prompt: {input_phrase}\\n #context: {context_phrase}\"}], tokenize=False, add_generation_prompt=True)\n\n# Generate SQL query\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, num_beams=1, temperature=0.3, top_k=50, top_p=0.95, max_time=180)\n\n# Print the result\ngenerated_text = outputs[0]['generated_text'][len(prompt):].strip()\nprint(f\"Generated SQL Query:\\n{generated_text}\")","metadata":{"_uuid":"4d35182b-99dc-4666-9b32-bce5f571a00e","_cell_guid":"ed0e3b07-84a5-4df9-b541-287d0cf7d5f4","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(prompt)","metadata":{"_uuid":"b6f29bf4-e43d-4fa3-808e-1694315750ab","_cell_guid":"c2cfb182-d3db-4881-b10b-2cf2ea8cd414","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}